{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Quick_demo_of_HuggingFace_version_of_Vision_Transformer_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czHcRYWitu-6"
   },
   "source": [
    "Adapted from Quick demo: Vision Transformer (ViT) by Google Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:21:40 INFO:Logger set up\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "logger.info(\"Logger set up\")\n",
    "\n",
    "import dataloader\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(123)\n",
    "num_examples = None\n",
    "patch_height, patch_width = 16, 16\n",
    "max_patches = 1200\n",
    "image_encoder_path = \"google/pix2struct-textcaps-base\"\n",
    "lm_path = \"mistralai/Mistral-7B-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Preprocess Data\n",
    "Mark all pixels that belongs to the bounding boxes of positive candidates as targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:21:42 INFO:PyTorch version 2.0.1 available.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e185771e40674d3c9b830f6774f70410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['action_uid', 'raw_html', 'cleaned_html', 'operation', 'pos_candidates', 'neg_candidates', 'website', 'domain', 'subdomain', 'annotation_id', 'confirmed_task', 'screenshot', 'action_reprs'],\n",
      "    num_rows: 7775\n",
      "})\n",
      "Dataset({\n",
      "    features: ['action_uid', 'operation', 'pos_candidates', 'website', 'domain', 'subdomain', 'annotation_id', 'confirmed_task', 'screenshot', 'previous_actions'],\n",
      "    num_rows: 7147\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# logger.info(f\"Use model {cfg.model.pretrained_model_name_or_path}\")\n",
    "# output_dir = HydraConfig.get().runtime.output_dir\n",
    "if num_examples:\n",
    "    train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(num_examples))\n",
    "else:\n",
    "    train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\")\n",
    "print(train_dataset)\n",
    "train_dataset = train_dataset.remove_columns([\"neg_candidates\", \"raw_html\", \"cleaned_html\"])\n",
    "train_dataset = dataloader.get_previous_actions(train_dataset)\n",
    "# filter out those without pos_candidates\n",
    "train_dataset = train_dataset.filter(lambda x: len(x)==1, input_columns=['pos_candidates'])\n",
    "train_dataset = train_dataset.remove_columns(['action_reprs'])\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prompt and label\n",
    "The full prompt is:\n",
    "\n",
    "[patch embeddings] \\n Based on the webpage screenshot, try to complete the following task:\\n Task: [task] \\n Previous actions:\\n [actions] \\n Which image patch contains the element to interact with next?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a229974a2b1841548e43cfb983b0e72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'screenshot': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x5429>,\n",
       " 'question': 'Based on the webpage screenshot, try to complete the following task:\\nTask: rent a car in Brooklyn - Central, NY on from April 9 to April 15.\\nRequired actions:[div]  Brooklyn - Central (New York), US -> CLICK\\nWhat should be the element to interact with given the required action?',\n",
       " 'boxes': [[114.59375, 365.1875, 306.8125, 25.6875]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_remove = set(train_dataset.column_names)\n",
    "cols_to_remove.remove(\"screenshot\")\n",
    "train_dataset = train_dataset.map(\n",
    "    dataloader.get_prompt_target,\n",
    "    batched=False,\n",
    "    remove_columns=list(cols_to_remove)\n",
    ")\n",
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out those with bounding box out of range\n",
    "\n",
    "# def box_in_range(example):\n",
    "#     print(example)\n",
    "#     l, b, _, _, = example[\"boxes\"]\n",
    "#     # width, height = example[\"screenshot\"].size\n",
    "#     width = height = 100\n",
    "#     return l < width and b < height\n",
    "    \n",
    "# train_dataset = train_dataset.filter(lambda x: x, input_columns=['valid'])\n",
    "# train_dataset = train_dataset.remove_columns(['valid'])\n",
    "# train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee7e048bb304f44964f2f58e63a3fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'screenshot': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x5429 at 0x7F34AC2ECED0>, 'input_ids': [1, 17158, 356, 272, 4686, 3005, 18794, 10672, 28725, 1464, 298, 4160, 272, 2296, 3638, 28747, 13, 4818, 28747, 7358, 264, 1253, 297, 21491, 387, 7993, 28725, 11800, 356, 477, 3999, 28705, 28774, 298, 3999, 28705, 28740, 28782, 28723, 13, 10135, 6768, 28747, 28792, 26771, 28793, 28705, 334, 1087, 3193, 12296, 14595, 13, 3195, 1023, 347, 272, 2442, 298, 14113, 395, 2078, 272, 3030, 2992, 28804, 733, 7637, 28793], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [[283.1875, 220.390625, 93.59375, 33.0]]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:22:28 INFO:Use device gpu\n",
      "08:22:28 INFO:Training data size 6789\n",
      "08:22:28 INFO:Eval data size 358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'screenshot': tensor([[ 1.0000,  1.0000,  0.9416,  ..., -1.3588, -1.3585, -1.3591],\n",
      "        [ 1.0000,  2.0000,  0.8819,  ..., -1.3587, -1.3581, -1.3591],\n",
      "        [ 1.0000,  3.0000,  0.9452,  ..., -1.3591, -1.3591, -1.3591],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]), 'input_ids': [1, 17158, 356, 272, 4686, 3005, 18794, 10672, 28725, 1464, 298, 4160, 272, 2296, 3638, 28747, 13, 4818, 28747, 8734, 889, 264, 6594, 524, 28762, 28741, 22989, 2708, 395, 272, 22989, 1474, 28705, 28740, 28734, 28734, 28734, 28734, 28734, 28734, 28734, 28734, 28740, 317, 1704, 282, 2696, 28705, 28740, 28734, 28734, 28750, 28770, 13, 10135, 6768, 28747, 28792, 2403, 28793, 28705, 4515, 9272, 7277, 7497, 8369, 2854, 3193, 12296, 14595, 13, 3195, 1023, 347, 272, 2442, 298, 14113, 395, 2078, 272, 3030, 2992, 28804, 733, 7637, 28793], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [[141.375, 5.136244867487869, 24.862499999999997, 8.56040811247978]], 'attention_mask_image': tensor([1., 1., 1.,  ..., 0., 0., 0.]), 'row_col': [86, 13]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Pix2StructImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(lm_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token # should be ok for casual LM\n",
    "processor = Pix2StructImageProcessor.from_pretrained(image_encoder_path) # TODO: define this somewhere else\n",
    "processor.max_patches = max_patches\n",
    "\n",
    "cols = train_dataset.column_names\n",
    "cols.remove(\"screenshot\")\n",
    "train_dataset = train_dataset.map(\n",
    "    dataloader.get_tokenize_fn(tokenizer),\n",
    "    remove_columns=cols,\n",
    "    )\n",
    "# train_dataset.set_format(\"pt\", columns=[\"input_ids\", \"attention_mask\", \"label\"], output_all_columns=True)\n",
    "print(train_dataset[0])\n",
    "train_dataset.set_transform(dataloader.get_preprocess_image_fn(processor, max_patches, patch_height, patch_width), output_all_columns=True) # process images on the fly\n",
    "# split the train_dataset into train and validation\n",
    "dataset = train_dataset.train_test_split(test_size=0.05) \n",
    "train_dataset, eval_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "print(train_dataset[0])\n",
    "logger.info(f\"Use device {'gpu' if torch.cuda.is_available() else 'cpu'}\")\n",
    "# logger.info(f\"Use batch size {cfg.train.batch_size}\")\n",
    "logger.info(f\"Training data size {len(train_dataset)}\")\n",
    "logger.info(f\"Eval data size {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df811c4ac789404c9cf51419fc7718b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afb03d3edd54969921d9b939b23aaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15401342976\n",
      "Layers and their dimensions:\n",
      "image_encoder.embeddings.patch_projection: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.0.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.0.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.0.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.0.attention.output: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.0.mlp.wi_0: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.0.mlp.wi_1: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.0.mlp.wo: torch.Size([768, 2048])\n",
      "image_encoder.encoder.layer.1.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.1.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.1.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.1.attention.output: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.1.mlp.wi_0: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.1.mlp.wi_1: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.1.mlp.wo: torch.Size([768, 2048])\n",
      "image_encoder.encoder.layer.2.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.2.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.2.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.2.attention.output: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.2.mlp.wi_0: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.2.mlp.wi_1: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.2.mlp.wo: torch.Size([768, 2048])\n",
      "image_encoder.encoder.layer.3.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.3.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.3.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.3.attention.output: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.3.mlp.wi_0: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.3.mlp.wi_1: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.3.mlp.wo: torch.Size([768, 2048])\n",
      "image_encoder.encoder.layer.4.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.4.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.4.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.4.attention.output: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.4.mlp.wi_0: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.4.mlp.wi_1: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.4.mlp.wo: torch.Size([768, 2048])\n",
      "image_encoder.encoder.layer.5.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.5.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.5.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.5.attention.output: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.5.mlp.wi_0: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.5.mlp.wi_1: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.5.mlp.wo: torch.Size([768, 2048])\n",
      "image_encoder.encoder.layer.6.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.6.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.6.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.6.attention.output: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.6.mlp.wi_0: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.6.mlp.wi_1: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.6.mlp.wo: torch.Size([768, 2048])\n",
      "image_encoder.encoder.layer.7.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.7.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.7.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.7.attention.output: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.7.mlp.wi_0: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.7.mlp.wi_1: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.7.mlp.wo: torch.Size([768, 2048])\n",
      "image_encoder.encoder.layer.8.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.8.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.8.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.8.attention.output: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.8.mlp.wi_0: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.8.mlp.wi_1: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.8.mlp.wo: torch.Size([768, 2048])\n",
      "image_encoder.encoder.layer.9.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.9.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.9.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.9.attention.output: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.9.mlp.wi_0: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.9.mlp.wi_1: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.9.mlp.wo: torch.Size([768, 2048])\n",
      "image_encoder.encoder.layer.10.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.10.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.10.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.10.attention.output: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.10.mlp.wi_0: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.10.mlp.wi_1: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.10.mlp.wo: torch.Size([768, 2048])\n",
      "image_encoder.encoder.layer.11.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.11.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.11.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.11.attention.output: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.11.mlp.wi_0: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.11.mlp.wi_1: torch.Size([2048, 768])\n",
      "image_encoder.encoder.layer.11.mlp.wo: torch.Size([768, 2048])\n",
      "projector: torch.Size([4096, 768])\n",
      "lm.model.layers.0.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.0.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.0.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.0.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.0.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.0.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.0.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.1.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.1.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.1.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.1.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.1.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.1.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.1.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.2.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.2.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.2.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.2.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.2.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.2.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.2.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.3.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.3.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.3.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.3.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.3.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.3.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.3.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.4.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.4.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.4.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.4.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.4.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.4.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.4.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.5.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.5.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.5.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.5.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.5.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.5.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.5.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.6.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.6.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.6.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.6.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.6.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.6.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.6.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.7.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.7.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.7.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.7.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.7.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.7.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.7.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.8.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.8.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.8.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.8.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.8.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.8.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.8.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.9.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.9.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.9.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.9.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.9.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.9.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.9.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.10.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.10.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.10.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.10.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.10.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.10.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.10.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.11.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.11.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.11.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.11.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.11.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.11.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.11.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.12.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.12.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.12.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.12.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.12.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.12.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.12.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.13.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.13.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.13.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.13.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.13.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.13.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.13.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.14.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.14.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.14.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.14.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.14.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.14.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.14.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.15.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.15.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.15.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.15.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.15.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.15.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.15.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.16.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.16.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.16.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.16.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.16.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.16.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.16.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.17.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.17.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.17.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.17.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.17.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.17.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.17.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.18.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.18.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.18.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.18.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.18.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.18.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.18.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.19.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.19.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.19.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.19.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.19.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.19.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.19.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.20.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.20.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.20.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.20.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.20.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.20.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.20.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.21.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.21.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.21.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.21.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.21.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.21.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.21.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.22.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.22.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.22.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.22.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.22.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.22.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.22.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.23.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.23.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.23.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.23.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.23.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.23.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.23.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.24.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.24.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.24.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.24.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.24.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.24.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.24.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.25.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.25.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.25.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.25.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.25.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.25.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.25.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.26.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.26.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.26.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.26.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.26.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.26.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.26.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.27.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.27.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.27.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.27.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.27.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.27.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.27.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.28.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.28.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.28.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.28.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.28.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.28.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.28.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.29.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.29.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.29.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.29.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.29.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.29.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.29.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.30.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.30.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.30.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.30.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.30.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.30.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.30.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.31.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.31.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.31.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.31.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.31.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.31.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.31.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.lm_head: torch.Size([32000, 4096])\n"
     ]
    }
   ],
   "source": [
    "import multimodal\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers import Pix2StructForConditionalGeneration, Pix2StructVisionConfig\n",
    "\n",
    "### Config for notebook\n",
    "config = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "config.return_dict = True\n",
    "config.use_cache = False\n",
    "config.low_cpu_mem_usage = True\n",
    "config.rope_theta = 10000.0\n",
    "config.attn_implementation = \"flash_attention_2\"\n",
    "###\n",
    "\n",
    "# TODO: Move config to somewhere else\n",
    "\n",
    "# image_encoder_config = Pix2StructVisionConfig.from_pretrained(\"google/pix2struct-base\")\n",
    "# TODO: try different hidden size?\n",
    "# image_encoder_config.seq_len = 27145\n",
    "# image_encoder_config.patch_size = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# image_encoder = Pix2StructVisionModel.from_pretrained(\"google/pix2struct-textcaps-base\", config=image_encoder_config, torch_dtype=torch.bfloat16)\n",
    "# image_encoder.to(device)\n",
    "\n",
    "image_encoder_config = AutoConfig.from_pretrained(image_encoder_path)\n",
    "image_encoder = Pix2StructForConditionalGeneration.from_pretrained(image_encoder_path, config=image_encoder_config).encoder\n",
    "image_encoder.to(device)\n",
    "\n",
    "lm = AutoModelForCausalLM.from_pretrained(lm_path, config=config, torch_dtype=torch.bfloat16)\n",
    "lm.to(device)\n",
    "\n",
    "model = multimodal.MultimodalAgent(config, image_encoder, lm, patch_width, patch_height)\n",
    "model.to(device)\n",
    "print(torch.cuda.memory_allocated())\n",
    "\n",
    "print(\"Layers and their dimensions:\")\n",
    "import torch.nn as nn\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "        print(f\"{name}: {module.weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 48,496,640 || all params: 7,385,214,464 || trainable%: 0.656672060593527\n",
      "base_model.model.image_encoder.embeddings.patch_projection.lora_A.default.weight\n",
      "base_model.model.image_encoder.embeddings.patch_projection.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.output.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.output.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.mlp.wi_0.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.mlp.wi_0.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.mlp.wi_1.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.mlp.wi_1.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.mlp.wo.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.mlp.wo.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.output.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.output.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.mlp.wi_0.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.mlp.wi_0.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.mlp.wi_1.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.mlp.wi_1.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.mlp.wo.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.mlp.wo.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.output.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.output.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.mlp.wi_0.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.mlp.wi_0.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.mlp.wi_1.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.mlp.wi_1.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.mlp.wo.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.mlp.wo.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.output.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.output.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.mlp.wi_0.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.mlp.wi_0.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.mlp.wi_1.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.mlp.wi_1.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.mlp.wo.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.mlp.wo.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.output.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.output.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.mlp.wi_0.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.mlp.wi_0.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.mlp.wi_1.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.mlp.wi_1.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.mlp.wo.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.mlp.wo.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.output.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.output.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.mlp.wi_0.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.mlp.wi_0.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.mlp.wi_1.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.mlp.wi_1.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.mlp.wo.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.mlp.wo.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.output.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.output.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.mlp.wi_0.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.mlp.wi_0.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.mlp.wi_1.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.mlp.wi_1.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.mlp.wo.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.mlp.wo.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.output.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.output.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.mlp.wi_0.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.mlp.wi_0.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.mlp.wi_1.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.mlp.wi_1.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.mlp.wo.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.mlp.wo.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.output.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.output.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.mlp.wi_0.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.mlp.wi_0.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.mlp.wi_1.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.mlp.wi_1.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.mlp.wo.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.mlp.wo.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.output.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.output.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.mlp.wi_0.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.mlp.wi_0.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.mlp.wi_1.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.mlp.wi_1.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.mlp.wo.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.mlp.wo.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.output.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.output.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.mlp.wi_0.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.mlp.wi_0.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.mlp.wi_1.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.mlp.wi_1.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.mlp.wo.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.mlp.wo.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.output.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.output.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.mlp.wi_0.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.mlp.wi_0.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.mlp.wi_1.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.mlp.wi_1.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.mlp.wo.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.mlp.wo.lora_B.default.weight\n",
      "base_model.model.projector.modules_to_save.default.weight\n",
      "base_model.model.projector.modules_to_save.default.bias\n",
      "base_model.model.lm.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.lm_head.lora_A.default.weight\n",
      "base_model.model.lm.lm_head.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_int8_training\n",
    "lora_config = LoraConfig(\n",
    "    # task_type=TaskType.CAUSAL_LM, # task type is not necessary, but this is needed to get the label\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.05,\n",
    "    target_modules=\"all-linear\",\n",
    "    modules_to_save=[\"projector\"] # this layer is not pretrained\n",
    ")\n",
    "\n",
    "# model.lm.enable_input_require_grads()\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "config = {\n",
    "        'lora_config': lora_config,\n",
    "        'learning_rate': 3e-5,\n",
    "        'num_train_epochs': 3,\n",
    "        'gradient_accumulation_steps': 16,\n",
    "        'per_device_train_batch_size': 1,\n",
    "        'per_device_eval_batch_size': 1,\n",
    "        'eval_accumulation_steps': 32,\n",
    "        'gradient_checkpointing': True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:22:40 WARNING:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Using auto half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 6,789\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 1,272\n",
      "  Number of trainable parameters = 48,496,640\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultimodalAgent' object has no attribute 'static_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/peft/peft_model.py:530\u001b[0m, in \u001b[0;36mPeftModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PeftModel' object has no attribute 'static_graph'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/peft/tuners/lora/model.py:267\u001b[0m, in \u001b[0;36mLoraModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LoraModel' object has no attribute 'static_graph'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m\n\u001b[1;32m     20\u001b[0m trainer \u001b[38;5;241m=\u001b[39m multimodal\u001b[38;5;241m.\u001b[39mMultimodalTrainer(\n\u001b[1;32m     21\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     22\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mmultimodal\u001b[38;5;241m.\u001b[39mcustom_collate,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/transformers/trainer.py:1869\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1869\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1872\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1875\u001b[0m ):\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/transformers/trainer.py:2768\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2767\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2768\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2771\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/multimodal.py:49\u001b[0m, in \u001b[0;36mMultimodalTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     46\u001b[0m     \n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# with torch.autocast(device_type=\"cuda\"):\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     device \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_graph\u001b[49m:\n\u001b[1;32m     50\u001b[0m         model\u001b[38;5;241m.\u001b[39mfind_unused_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         model\u001b[38;5;241m.\u001b[39m_set_static_graph()\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/peft/peft_model.py:532\u001b[0m, in \u001b[0;36mPeftModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name)  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model, name)\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/peft/tuners/lora/model.py:269\u001b[0m, in \u001b[0;36mLoraModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name)  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, name)\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultimodalAgent' object has no attribute 'static_graph'"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    overwrite_output_dir=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    bf16=True,  # Use BF16 for flash attention\n",
    "    # evlaution\n",
    "    label_names=[\"labels\"], # so that trainer will call compute_loss\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    include_inputs_for_metrics=True,\n",
    "    log_level=\"info\",\n",
    "    # logging strategies\n",
    "    logging_dir=f\"output/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    "    remove_unused_columns=False,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ") # TODO: move train arguments to config\n",
    "trainer = multimodal.MultimodalTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=multimodal.compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=multimodal.custom_collate,\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pix2Struct, reference: https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Pix2StructVisionModel\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"google/pix2struct-base\")\n",
    "# model = Pix2StructVisionModel.from_pretrained(\"google/pix2struct-base\")\n",
    "train_dataset[997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor, Pix2StructVisionModel\n",
    "\n",
    "\n",
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(10))\n",
    "image = train_dataset[3][\"screenshot\"]\n",
    "\n",
    "text = \"A picture of\"\n",
    "\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-textcaps-base\")\n",
    "processor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n",
    "# processor = Pix2StructImageProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n",
    "\n",
    "# image only\n",
    "inputs = processor(images=train_dataset[\"screenshot\"], text=text, return_tensors=\"pt\")\n",
    "print(inputs.keys())\n",
    "predictions = model.generate(**inputs)\n",
    "print(processor.decode(predictions[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pix2Struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pix2StructImageProcessor, Pix2StructVisionModel, Pix2StructConfig, Pix2StructForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(10))\n",
    "image = train_dataset[3][\"screenshot\"]\n",
    "\n",
    "# TODO: Move config to somewhere else\n",
    "image_encoder_path = \"google/pix2struct-textcaps-base\"\n",
    "image_encoder_config = Pix2StructConfig.from_pretrained(image_encoder_path)\n",
    "# TODO: try different hidden size?\n",
    "max_patches = 2000\n",
    "patch_size = 16\n",
    "# image_encoder_config.vision_config.seq_len = max_patches\n",
    "# image_encoder_config.vision_config.patch_size = patch_size\n",
    "print(image_encoder_config)\n",
    "\n",
    "image_encoder = Pix2StructForConditionalGeneration.from_pretrained(image_encoder_path, config=image_encoder_config).encoder\n",
    "print(image_encoder)\n",
    "image_encoder.to(device)\n",
    "\n",
    "processor = Pix2StructImageProcessor.from_pretrained(image_encoder_path) # TODO: define this somewhere else\n",
    "processor.max_patches = max_patches\n",
    "processor.patch_size = {\"height\":patch_size, \"width\":patch_size}\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "print(image_encoder(**inputs))\n",
    "print(torch.cuda.memory_summary())\n",
    "# 2000 -> 7G\n",
    "# 3000 -> 14G\n",
    "# 4000 -> 25G\n",
    "# 5000 -> 37G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoImageProcessor, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(10))\n",
    "image = train_dataset[3][\"screenshot\"]\n",
    "\n",
    "# TODO: Move config to somewhere else\n",
    "image_encoder_path = \"google/vit-base-patch16-224\"\n",
    "image_encoder_config = AutoConfig.from_pretrained(image_encoder_path)\n",
    "# TODO: try different hidden size?\n",
    "# print(image_encoder_config)\n",
    "\n",
    "image_encoder = AutoModel.from_pretrained(image_encoder_path, config=image_encoder_config)\n",
    "# print(image_encoder)\n",
    "image_encoder.to(device)\n",
    "\n",
    "downscale_factor = 4\n",
    "processor = AutoImageProcessor.from_pretrained(image_encoder_path) # TODO: define this somewhere else\n",
    "processor.size = {\"height\":5429//downscale_factor, \"width\":1280//downscale_factor}\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "print(inputs.pixel_values.shape)\n",
    "plt.figure(figsize=(12, 40))\n",
    "plt.imshow(inputs.pixel_values.cpu()[0].permute((1,2,0)))\n",
    "plt.show()\n",
    "h = image_encoder(inputs[\"pixel_values\"], interpolate_pos_encoding=True).last_hidden_state\n",
    "# print(torch.cuda.memory_summary())\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match downscaled image patch index to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[3][\"pos_candidates\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match target index to patch index\n",
    "\n",
    "\n",
    "bounding_box_rect is in the format of (left, bottom, width, height), so pixel_values[:,bottom:bottom+height,left:left+width] should be marked as positive\n",
    "\n",
    "unscaled index 2d -> scaled index 2d -> patch index 2d -> patch index 1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortest width / height:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\")\n",
    "cands = train_dataset[\"pos_candidates\"]\n",
    "shortest = 100\n",
    "widths = []\n",
    "heights = []\n",
    "import json\n",
    "for cand_list in cands:\n",
    "    for cand in cand_list:\n",
    "        json_data = json.loads(cand)\n",
    "        attributes = json.loads(json_data['attributes'])\n",
    "        bounding_box_rect_str = attributes['bounding_box_rect']\n",
    "        lbwh = tuple(map(float, bounding_box_rect_str.split(',')))\n",
    "        widths.append(lbwh[2])\n",
    "        heights.append(lbwh[3])\n",
    "        # if lbwh[2] <= 0 or lbwh[3] <= 0:\n",
    "        #     print(cand_list)\n",
    "        #     print(shortest)\n",
    "\n",
    "        # shortest = min(shortest, lbwh[2], lbwh[3])\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "# plt.hist(widths, bins=100)\n",
    "# plt.show()\n",
    "heights = np.array(heights)\n",
    "plt.hist(heights[heights < 200], bins=100)\n",
    "plt.axvline(x=32, color='r', linestyle='--')\n",
    "plt.title(\"Pos candidates height\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "import torch\n",
    "sample = train_dataset[3]\n",
    "print(sample[\"pos_candidates\"])\n",
    "\n",
    "image = sample[\"screenshot\"]\n",
    "print(image.size)\n",
    "processor = ViTImageProcessor(size={\"height\": 5429, \"width\": 1280})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values # [1, 3, 224, 224]\n",
    "print(pixel_values.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure()\n",
    "plt.imshow(np.transpose(pixel_values[0,:,410:410+46,96:96+106].cpu(), (1,2,0)))\n",
    "\n",
    "\n",
    "processor2 = ViTImageProcessor(size={\"height\": 5429//2, \"width\": 1280//2})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inputs2 = processor2(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values2 = inputs2.pixel_values # [1, 3, 224, 224]\n",
    "print(pixel_values2.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure()\n",
    "plt.imshow(np.transpose(pixel_values2[0,:,410//2:(410+46)//2,96//2:(96+106)//2].cpu(), (1,2,0)))\n",
    "\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.imshow(np.transpose(pixel_values[0].cpu(), (1,2,0)))\n",
    "# for i in range(0, 1000, 100):\n",
    "#     plt.figure()\n",
    "#     plt.imshow(np.transpose(pixel_values[0,:,i:i+160,i:i+160].cpu(), (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for i in range(16):\n",
    "#     print([i*16+j for j in range(16)])\n",
    "print(boxes_to_patch_idx_multitarget([96,410.390625,106,46], 640//16))\n",
    "boxes_to_patch_idx([96,410.390625,106,46], 640//16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03c0d0f4f4de45dcb6364473147606b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10bc8d5b2bf14502abf97c47a27ee2b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b59827d64e84940b15ac6e47f5d2e05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23b582e617554b7a874d246e48adf7f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b536d7f4e7b40b38db4c583fcf01a2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2d5cfaa9cbd3499ea81b87e4a2b3ef52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48901010590e483da4289444ec48c40a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d5cfaa9cbd3499ea81b87e4a2b3ef52",
      "placeholder": "",
      "style": "IPY_MODEL_f06d20046f89484e9180562d0a115aee",
      "value": " 346M/346M [00:03&lt;00:00, 93.3MB/s]"
     }
    },
    "62305803798b4130899d2e93a3b1a5e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03c0d0f4f4de45dcb6364473147606b0",
      "max": 346351599,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b536d7f4e7b40b38db4c583fcf01a2f",
      "value": 346351599
     }
    },
    "825a988043724380a053616c711e8401": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23b582e617554b7a874d246e48adf7f7",
      "placeholder": "",
      "style": "IPY_MODEL_c734ae403585464ba94cdf0f37b41ac4",
      "value": " 69.7k/69.7k [00:00&lt;00:00, 1.64MB/s]"
     }
    },
    "8b131288e18946a19cd649f6ca43d2a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "97c5e47863024e3b9a1fb13b9e8868d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98ba59932d084e008bd851cb96927063": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d87d89e25d1346a29fb19bbc9fcffa85",
      "placeholder": "",
      "style": "IPY_MODEL_ff202effd8964c98b8e2fc5c52264d68",
      "value": "Downloading ()lve/main/config.json: 100%"
     }
    },
    "9ff2ca6160cd4ad283657bceb6831691": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10bc8d5b2bf14502abf97c47a27ee2b7",
      "max": 69665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b131288e18946a19cd649f6ca43d2a9",
      "value": 69665
     }
    },
    "c734ae403585464ba94cdf0f37b41ac4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d09292f5afc045cba07318db5e171471": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d87d89e25d1346a29fb19bbc9fcffa85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "deb144cf25e74807b3adb4fd0fc6ba26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f1c630d0443b44c484db793403db5713",
       "IPY_MODEL_62305803798b4130899d2e93a3b1a5e2",
       "IPY_MODEL_48901010590e483da4289444ec48c40a"
      ],
      "layout": "IPY_MODEL_d09292f5afc045cba07318db5e171471"
     }
    },
    "e7d1c5825ecc466493d5da9633b635c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f06d20046f89484e9180562d0a115aee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f1c630d0443b44c484db793403db5713": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7d1c5825ecc466493d5da9633b635c8",
      "placeholder": "",
      "style": "IPY_MODEL_97c5e47863024e3b9a1fb13b9e8868d9",
      "value": "Downloading ()&quot;pytorch_model.bin&quot;;: 100%"
     }
    },
    "f615c90dfc314e0294b2e32b81f07b5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_98ba59932d084e008bd851cb96927063",
       "IPY_MODEL_9ff2ca6160cd4ad283657bceb6831691",
       "IPY_MODEL_825a988043724380a053616c711e8401"
      ],
      "layout": "IPY_MODEL_1b59827d64e84940b15ac6e47f5d2e05"
     }
    },
    "ff202effd8964c98b8e2fc5c52264d68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
