{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Quick_demo_of_HuggingFace_version_of_Vision_Transformer_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czHcRYWitu-6"
   },
   "source": [
    "Adapted from Quick demo: Vision Transformer (ViT) by Google Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:47:15 INFO:Logger set up\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "logger.info(\"Logger set up\")\n",
    "\n",
    "import dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(123)\n",
    "num_examples = 1000\n",
    "patch_height, patch_width = 16, 16\n",
    "max_patches = 4000\n",
    "image_encoder_path = \"google/vit-base-patch16-224\"\n",
    "lm_path = \"mistralai/Mistral-7B-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Preprocess Data\n",
    "Mark all pixels that belongs to the bounding boxes of positive candidates as targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:47:16 INFO:PyTorch version 2.0.1 available.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9a2d954af64ae993a16bfca0df8276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['action_uid', 'raw_html', 'cleaned_html', 'operation', 'pos_candidates', 'neg_candidates', 'website', 'domain', 'subdomain', 'annotation_id', 'confirmed_task', 'screenshot', 'action_reprs'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['action_uid', 'operation', 'pos_candidates', 'website', 'domain', 'subdomain', 'annotation_id', 'confirmed_task', 'screenshot', 'previous_actions'],\n",
      "    num_rows: 892\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# logger.info(f\"Use model {cfg.model.pretrained_model_name_or_path}\")\n",
    "# output_dir = HydraConfig.get().runtime.output_dir\n",
    "if num_examples:\n",
    "    train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(num_examples))\n",
    "else:\n",
    "    train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\")\n",
    "print(train_dataset)\n",
    "train_dataset = train_dataset.remove_columns([\"neg_candidates\", \"raw_html\", \"cleaned_html\"])\n",
    "train_dataset = dataloader.get_previous_actions(train_dataset)\n",
    "# filter out those without pos_candidates\n",
    "train_dataset = train_dataset.filter(lambda x: len(x)==1, input_columns=['pos_candidates'])\n",
    "train_dataset = train_dataset.remove_columns(['action_reprs'])\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prompt and label\n",
    "The full prompt is:\n",
    "\n",
    "[patch embeddings] \\n Based on the webpage screenshot, try to complete the following task:\\n Task: [task] \\n Previous actions:\\n [actions] \\n Which image patch contains the element to interact with next?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86005d46bb504e778d4d52d6f40847e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/892 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'screenshot': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x5429>,\n",
       " 'question': 'Based on the HTML webpage, try to complete the following task:\\nTask: rent a car in Brooklyn - Central, NY on from April 9 to April 15.\\nPrevious actions:\\n[heading]  CAR -> CLICK\\n[combobox]  Enter pick up city, airport name, or airport code. -> TYPE: Brooklyn Central\\nWhat should be the element to interact with next?',\n",
       " 'boxes': [[114.59375, 365.1875, 306.8125, 25.6875]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_remove = set(train_dataset.column_names)\n",
    "cols_to_remove.remove(\"screenshot\")\n",
    "train_dataset = train_dataset.map(\n",
    "    dataloader.get_prompt_target,\n",
    "    batched=False,\n",
    "    remove_columns=list(cols_to_remove)\n",
    ")\n",
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out those with bounding box out of range\n",
    "\n",
    "# def box_in_range(example):\n",
    "#     print(example)\n",
    "#     l, b, _, _, = example[\"boxes\"]\n",
    "#     # width, height = example[\"screenshot\"].size\n",
    "#     width = height = 100\n",
    "#     return l < width and b < height\n",
    "    \n",
    "# train_dataset = train_dataset.filter(lambda x: x, input_columns=['valid'])\n",
    "# train_dataset = train_dataset.remove_columns(['valid'])\n",
    "# train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d40c8765794abe9d6c55f90b027611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/892 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'screenshot': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x5429 at 0x7F5981BD8F50>, 'input_ids': [1, 17158, 356, 272, 13987, 4686, 3005, 28725, 1464, 298, 4160, 272, 2296, 3638, 28747, 13, 4818, 28747, 7358, 264, 1253, 297, 21491, 387, 7993, 28725, 11800, 356, 477, 3999, 28705, 28774, 298, 3999, 28705, 28740, 28782, 28723, 13, 28284, 6768, 28747, 13, 5364, 13, 3195, 1023, 347, 272, 2442, 298, 14113, 395, 1679, 28804, 733, 7637, 28793], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [[283.1875, 220.390625, 93.59375, 33.0]]}\n",
      "{'screenshot': tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]]]), 'input_ids': [1, 17158, 356, 272, 13987, 4686, 3005, 28725, 1464, 298, 4160, 272, 2296, 3638, 28747, 13, 4818, 28747, 19332, 331, 20654, 6138, 752, 28710, 28733, 7971, 9922, 18000, 438, 1611, 28723, 13, 28284, 6768, 28747, 13, 28792, 2403, 28793, 28705, 9875, 17102, 2255, 3193, 12203, 3491, 13, 28792, 2403, 28793, 28705, 12089, 1799, 6138, 10615, 3193, 12296, 14595, 13, 28792, 14244, 28793, 259, 3193, 12296, 14595, 13, 28792, 1538, 28793, 259, 3193, 12296, 14595, 13, 3195, 1023, 347, 272, 2442, 298, 14113, 395, 1679, 28804, 733, 7637, 28793], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [[204.74375, 498.8805324459235, 80.71875, 20.445923460898502]]}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m train_dataset, eval_dataset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_dataset[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 20\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# logger.info(f\"Use batch size {cfg.train.batch_size}\")\u001b[39;00m\n\u001b[1;32m     22\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(lm_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token # should be ok for casual LM\n",
    "processor = AutoImageProcessor.from_pretrained(image_encoder_path)\n",
    "\n",
    "cols = train_dataset.column_names\n",
    "cols.remove(\"screenshot\")\n",
    "train_dataset = train_dataset.map(\n",
    "    dataloader.get_tokenize_fn(tokenizer),\n",
    "    remove_columns=cols,\n",
    "    )\n",
    "# train_dataset.set_format(\"pt\", columns=[\"input_ids\", \"attention_mask\", \"label\"], output_all_columns=True)\n",
    "print(train_dataset[0])\n",
    "train_dataset.set_transform(dataloader.get_preprocess_image_fn(processor, max_patches, patch_height, patch_width), output_all_columns=True) # process images on the fly\n",
    "# split the train_dataset into train and validation\n",
    "dataset = train_dataset.train_test_split(test_size=0.05) \n",
    "train_dataset, eval_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "print(train_dataset[0])\n",
    "logger.info(f\"Use device {'gpu' if torch.cuda.is_available() else 'cpu'}\")\n",
    "# logger.info(f\"Use batch size {cfg.train.batch_size}\")\n",
    "logger.info(f\"Training data size {len(train_dataset)}\")\n",
    "logger.info(f\"Eval data size {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019dba63ac434f58a7e5af85a71bfd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ac0180a3ea4a55b93e2446104b4ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15378507776\n",
      "Layers and their dimensions:\n",
      "image_encoder.embeddings.patch_embeddings.projection: torch.Size([768, 3, 16, 16])\n",
      "image_encoder.encoder.layer.0.attention.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.0.attention.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.0.attention.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.0.attention.output.dense: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.0.intermediate.dense: torch.Size([3072, 768])\n",
      "image_encoder.encoder.layer.0.output.dense: torch.Size([768, 3072])\n",
      "image_encoder.encoder.layer.1.attention.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.1.attention.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.1.attention.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.1.attention.output.dense: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.1.intermediate.dense: torch.Size([3072, 768])\n",
      "image_encoder.encoder.layer.1.output.dense: torch.Size([768, 3072])\n",
      "image_encoder.encoder.layer.2.attention.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.2.attention.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.2.attention.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.2.attention.output.dense: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.2.intermediate.dense: torch.Size([3072, 768])\n",
      "image_encoder.encoder.layer.2.output.dense: torch.Size([768, 3072])\n",
      "image_encoder.encoder.layer.3.attention.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.3.attention.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.3.attention.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.3.attention.output.dense: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.3.intermediate.dense: torch.Size([3072, 768])\n",
      "image_encoder.encoder.layer.3.output.dense: torch.Size([768, 3072])\n",
      "image_encoder.encoder.layer.4.attention.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.4.attention.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.4.attention.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.4.attention.output.dense: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.4.intermediate.dense: torch.Size([3072, 768])\n",
      "image_encoder.encoder.layer.4.output.dense: torch.Size([768, 3072])\n",
      "image_encoder.encoder.layer.5.attention.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.5.attention.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.5.attention.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.5.attention.output.dense: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.5.intermediate.dense: torch.Size([3072, 768])\n",
      "image_encoder.encoder.layer.5.output.dense: torch.Size([768, 3072])\n",
      "image_encoder.encoder.layer.6.attention.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.6.attention.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.6.attention.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.6.attention.output.dense: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.6.intermediate.dense: torch.Size([3072, 768])\n",
      "image_encoder.encoder.layer.6.output.dense: torch.Size([768, 3072])\n",
      "image_encoder.encoder.layer.7.attention.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.7.attention.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.7.attention.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.7.attention.output.dense: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.7.intermediate.dense: torch.Size([3072, 768])\n",
      "image_encoder.encoder.layer.7.output.dense: torch.Size([768, 3072])\n",
      "image_encoder.encoder.layer.8.attention.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.8.attention.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.8.attention.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.8.attention.output.dense: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.8.intermediate.dense: torch.Size([3072, 768])\n",
      "image_encoder.encoder.layer.8.output.dense: torch.Size([768, 3072])\n",
      "image_encoder.encoder.layer.9.attention.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.9.attention.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.9.attention.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.9.attention.output.dense: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.9.intermediate.dense: torch.Size([3072, 768])\n",
      "image_encoder.encoder.layer.9.output.dense: torch.Size([768, 3072])\n",
      "image_encoder.encoder.layer.10.attention.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.10.attention.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.10.attention.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.10.attention.output.dense: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.10.intermediate.dense: torch.Size([3072, 768])\n",
      "image_encoder.encoder.layer.10.output.dense: torch.Size([768, 3072])\n",
      "image_encoder.encoder.layer.11.attention.attention.query: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.11.attention.attention.key: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.11.attention.attention.value: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.11.attention.output.dense: torch.Size([768, 768])\n",
      "image_encoder.encoder.layer.11.intermediate.dense: torch.Size([3072, 768])\n",
      "image_encoder.encoder.layer.11.output.dense: torch.Size([768, 3072])\n",
      "image_encoder.pooler.dense: torch.Size([768, 768])\n",
      "projector: torch.Size([4096, 768])\n",
      "lm.model.layers.0.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.0.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.0.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.0.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.0.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.0.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.0.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.1.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.1.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.1.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.1.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.1.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.1.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.1.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.2.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.2.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.2.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.2.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.2.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.2.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.2.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.3.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.3.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.3.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.3.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.3.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.3.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.3.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.4.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.4.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.4.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.4.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.4.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.4.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.4.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.5.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.5.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.5.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.5.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.5.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.5.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.5.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.6.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.6.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.6.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.6.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.6.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.6.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.6.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.7.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.7.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.7.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.7.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.7.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.7.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.7.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.8.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.8.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.8.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.8.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.8.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.8.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.8.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.9.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.9.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.9.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.9.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.9.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.9.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.9.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.10.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.10.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.10.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.10.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.10.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.10.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.10.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.11.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.11.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.11.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.11.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.11.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.11.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.11.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.12.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.12.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.12.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.12.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.12.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.12.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.12.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.13.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.13.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.13.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.13.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.13.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.13.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.13.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.14.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.14.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.14.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.14.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.14.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.14.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.14.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.15.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.15.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.15.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.15.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.15.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.15.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.15.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.16.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.16.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.16.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.16.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.16.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.16.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.16.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.17.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.17.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.17.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.17.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.17.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.17.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.17.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.18.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.18.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.18.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.18.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.18.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.18.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.18.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.19.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.19.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.19.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.19.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.19.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.19.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.19.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.20.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.20.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.20.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.20.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.20.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.20.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.20.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.21.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.21.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.21.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.21.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.21.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.21.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.21.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.22.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.22.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.22.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.22.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.22.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.22.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.22.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.23.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.23.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.23.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.23.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.23.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.23.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.23.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.24.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.24.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.24.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.24.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.24.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.24.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.24.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.25.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.25.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.25.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.25.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.25.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.25.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.25.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.26.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.26.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.26.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.26.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.26.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.26.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.26.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.27.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.27.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.27.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.27.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.27.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.27.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.27.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.28.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.28.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.28.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.28.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.28.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.28.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.28.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.29.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.29.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.29.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.29.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.29.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.29.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.29.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.30.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.30.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.30.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.30.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.30.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.30.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.30.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.model.layers.31.self_attn.q_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.31.self_attn.k_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.31.self_attn.v_proj: torch.Size([1024, 4096])\n",
      "lm.model.layers.31.self_attn.o_proj: torch.Size([4096, 4096])\n",
      "lm.model.layers.31.mlp.gate_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.31.mlp.up_proj: torch.Size([14336, 4096])\n",
      "lm.model.layers.31.mlp.down_proj: torch.Size([4096, 14336])\n",
      "lm.lm_head: torch.Size([32000, 4096])\n"
     ]
    }
   ],
   "source": [
    "import multimodal\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "import torch\n",
    "# from transformers import Pix2StructVisionModel, ViTImageProcessor, Pix2StructVisionConfig\n",
    "\n",
    "### Config for notebook\n",
    "config = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "config.return_dict = True\n",
    "config.use_cache = False\n",
    "config.low_cpu_mem_usage = True\n",
    "config.rope_theta = 10000.0\n",
    "config.attn_implementation = \"flash_attention_2\"\n",
    "###\n",
    "\n",
    "# TODO: Move config to somewhere else\n",
    "\n",
    "# image_encoder_config = Pix2StructVisionConfig.from_pretrained(\"google/pix2struct-base\")\n",
    "# TODO: try different hidden size?\n",
    "# image_encoder_config.seq_len = 27145\n",
    "# image_encoder_config.patch_size = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# image_encoder = Pix2StructVisionModel.from_pretrained(\"google/pix2struct-textcaps-base\", config=image_encoder_config, torch_dtype=torch.bfloat16)\n",
    "# image_encoder.to(device)\n",
    "\n",
    "image_encoder_config = AutoConfig.from_pretrained(image_encoder_path)\n",
    "image_encoder = AutoModel.from_pretrained(image_encoder_path, config=image_encoder_config)\n",
    "image_encoder.to(device)\n",
    "\n",
    "lm = AutoModelForCausalLM.from_pretrained(lm_path, config=config, torch_dtype=torch.bfloat16)\n",
    "lm.to(device)\n",
    "\n",
    "model = multimodal.MultimodalAgent(config, image_encoder, lm, patch_width, patch_height)\n",
    "model.to(device)\n",
    "print(torch.cuda.memory_allocated())\n",
    "\n",
    "print(\"Layers and their dimensions:\")\n",
    "import torch.nn as nn\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "        print(f\"{name}: {module.weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 48,349,184 || all params: 7,379,620,352 || trainable%: 0.6551716984586688\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.pooler.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.pooler.dense.lora_B.default.weight\n",
      "base_model.model.projector.modules_to_save.default.weight\n",
      "base_model.model.projector.modules_to_save.default.bias\n",
      "base_model.model.lm.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.lm_head.lora_A.default.weight\n",
      "base_model.model.lm.lm_head.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_int8_training\n",
    "lora_config = LoraConfig(\n",
    "    # task_type=TaskType.CAUSAL_LM, # task type is not necessary, but this is needed to get the label\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.05,\n",
    "    target_modules=\"all-linear\",\n",
    "    modules_to_save=[\"projector\"] # this layer is not pretrained\n",
    ")\n",
    "\n",
    "# model.lm.enable_input_require_grads()\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "config = {\n",
    "        'lora_config': lora_config,\n",
    "        'learning_rate': 1e-3,\n",
    "        'num_train_epochs': 1,\n",
    "        'gradient_accumulation_steps': 32,\n",
    "        'per_device_train_batch_size': 1,\n",
    "        'per_device_eval_batch_size': 1,\n",
    "        'eval_accumulation_steps': 32,\n",
    "        'gradient_checkpointing': True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:43:16 WARNING:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Using auto half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 847\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 32\n",
      "  Total optimization steps = 26\n",
      "  Number of trainable parameters = 48,349,184\n",
      "/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8/26 00:54 < 02:43, 0.11 it/s, Epoch 0.26/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    overwrite_output_dir=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    bf16=True,  # Use BF16 for flash attention\n",
    "    # evlaution\n",
    "    label_names=[\"labels\"], # so that trainer will call compute_loss\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    include_inputs_for_metrics=True,\n",
    "    log_level=\"info\",\n",
    "    # logging strategies\n",
    "    logging_dir=f\"output/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    remove_unused_columns=False,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ") # TODO: move train arguments to config\n",
    "trainer = multimodal.MultimodalTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=multimodal.compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=multimodal.custom_collate,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pix2Struct, reference: https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Pix2StructVisionModel\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"google/pix2struct-base\")\n",
    "# model = Pix2StructVisionModel.from_pretrained(\"google/pix2struct-base\")\n",
    "train_dataset[997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor, Pix2StructVisionModel\n",
    "\n",
    "\n",
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(10))\n",
    "image = train_dataset[3][\"screenshot\"]\n",
    "\n",
    "text = \"A picture of\"\n",
    "\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-textcaps-base\")\n",
    "processor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n",
    "# processor = Pix2StructImageProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n",
    "\n",
    "# image only\n",
    "inputs = processor(images=train_dataset[\"screenshot\"], text=text, return_tensors=\"pt\")\n",
    "print(inputs.keys())\n",
    "predictions = model.generate(**inputs)\n",
    "print(processor.decode(predictions[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pix2Struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pix2StructImageProcessor, Pix2StructVisionModel, Pix2StructConfig, Pix2StructForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(10))\n",
    "image = train_dataset[3][\"screenshot\"]\n",
    "\n",
    "# TODO: Move config to somewhere else\n",
    "image_encoder_path = \"google/pix2struct-textcaps-base\"\n",
    "image_encoder_config = Pix2StructConfig.from_pretrained(image_encoder_path)\n",
    "# TODO: try different hidden size?\n",
    "max_patches = 2000\n",
    "patch_size = 16\n",
    "# image_encoder_config.vision_config.seq_len = max_patches\n",
    "# image_encoder_config.vision_config.patch_size = patch_size\n",
    "print(image_encoder_config)\n",
    "\n",
    "image_encoder = Pix2StructForConditionalGeneration.from_pretrained(image_encoder_path, config=image_encoder_config).encoder\n",
    "print(image_encoder)\n",
    "image_encoder.to(device)\n",
    "\n",
    "processor = Pix2StructImageProcessor.from_pretrained(image_encoder_path) # TODO: define this somewhere else\n",
    "processor.max_patches = max_patches\n",
    "processor.patch_size = {\"height\":patch_size, \"width\":patch_size}\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "print(image_encoder(**inputs))\n",
    "print(torch.cuda.memory_summary())\n",
    "# 2000 -> 7G\n",
    "# 3000 -> 14G\n",
    "# 4000 -> 25G\n",
    "# 5000 -> 37G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoImageProcessor, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(10))\n",
    "image = train_dataset[3][\"screenshot\"]\n",
    "\n",
    "# TODO: Move config to somewhere else\n",
    "image_encoder_path = \"google/vit-base-patch16-224\"\n",
    "image_encoder_config = AutoConfig.from_pretrained(image_encoder_path)\n",
    "# TODO: try different hidden size?\n",
    "# print(image_encoder_config)\n",
    "\n",
    "image_encoder = AutoModel.from_pretrained(image_encoder_path, config=image_encoder_config)\n",
    "# print(image_encoder)\n",
    "image_encoder.to(device)\n",
    "\n",
    "downscale_factor = 4\n",
    "processor = AutoImageProcessor.from_pretrained(image_encoder_path) # TODO: define this somewhere else\n",
    "processor.size = {\"height\":5429//downscale_factor, \"width\":1280//downscale_factor}\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "print(inputs.pixel_values.shape)\n",
    "plt.figure(figsize=(12, 40))\n",
    "plt.imshow(inputs.pixel_values.cpu()[0].permute((1,2,0)))\n",
    "plt.show()\n",
    "h = image_encoder(inputs[\"pixel_values\"], interpolate_pos_encoding=True).last_hidden_state\n",
    "# print(torch.cuda.memory_summary())\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match downscaled image patch index to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[3][\"pos_candidates\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match target index to patch index\n",
    "\n",
    "\n",
    "bounding_box_rect is in the format of (left, bottom, width, height), so pixel_values[:,bottom:bottom+height,left:left+width] should be marked as positive\n",
    "\n",
    "unscaled index 2d -> scaled index 2d -> patch index 2d -> patch index 1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortest width / height:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\")\n",
    "cands = train_dataset[\"pos_candidates\"]\n",
    "shortest = 100\n",
    "widths = []\n",
    "heights = []\n",
    "import json\n",
    "for cand_list in cands:\n",
    "    for cand in cand_list:\n",
    "        json_data = json.loads(cand)\n",
    "        attributes = json.loads(json_data['attributes'])\n",
    "        bounding_box_rect_str = attributes['bounding_box_rect']\n",
    "        lbwh = tuple(map(float, bounding_box_rect_str.split(',')))\n",
    "        widths.append(lbwh[2])\n",
    "        heights.append(lbwh[3])\n",
    "        # if lbwh[2] <= 0 or lbwh[3] <= 0:\n",
    "        #     print(cand_list)\n",
    "        #     print(shortest)\n",
    "\n",
    "        # shortest = min(shortest, lbwh[2], lbwh[3])\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "# plt.hist(widths, bins=100)\n",
    "# plt.show()\n",
    "heights = np.array(heights)\n",
    "plt.hist(heights[heights < 200], bins=100)\n",
    "plt.axvline(x=32, color='r', linestyle='--')\n",
    "plt.title(\"Pos candidates height\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "import torch\n",
    "sample = train_dataset[3]\n",
    "print(sample[\"pos_candidates\"])\n",
    "\n",
    "image = sample[\"screenshot\"]\n",
    "print(image.size)\n",
    "processor = ViTImageProcessor(size={\"height\": 5429, \"width\": 1280})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values # [1, 3, 224, 224]\n",
    "print(pixel_values.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure()\n",
    "plt.imshow(np.transpose(pixel_values[0,:,410:410+46,96:96+106].cpu(), (1,2,0)))\n",
    "\n",
    "\n",
    "processor2 = ViTImageProcessor(size={\"height\": 5429//2, \"width\": 1280//2})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inputs2 = processor2(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values2 = inputs2.pixel_values # [1, 3, 224, 224]\n",
    "print(pixel_values2.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure()\n",
    "plt.imshow(np.transpose(pixel_values2[0,:,410//2:(410+46)//2,96//2:(96+106)//2].cpu(), (1,2,0)))\n",
    "\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.imshow(np.transpose(pixel_values[0].cpu(), (1,2,0)))\n",
    "# for i in range(0, 1000, 100):\n",
    "#     plt.figure()\n",
    "#     plt.imshow(np.transpose(pixel_values[0,:,i:i+160,i:i+160].cpu(), (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for i in range(16):\n",
    "#     print([i*16+j for j in range(16)])\n",
    "print(boxes_to_patch_idx_multitarget([96,410.390625,106,46], 640//16))\n",
    "boxes_to_patch_idx([96,410.390625,106,46], 640//16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03c0d0f4f4de45dcb6364473147606b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10bc8d5b2bf14502abf97c47a27ee2b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b59827d64e84940b15ac6e47f5d2e05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23b582e617554b7a874d246e48adf7f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b536d7f4e7b40b38db4c583fcf01a2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2d5cfaa9cbd3499ea81b87e4a2b3ef52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48901010590e483da4289444ec48c40a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d5cfaa9cbd3499ea81b87e4a2b3ef52",
      "placeholder": "",
      "style": "IPY_MODEL_f06d20046f89484e9180562d0a115aee",
      "value": " 346M/346M [00:03&lt;00:00, 93.3MB/s]"
     }
    },
    "62305803798b4130899d2e93a3b1a5e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03c0d0f4f4de45dcb6364473147606b0",
      "max": 346351599,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b536d7f4e7b40b38db4c583fcf01a2f",
      "value": 346351599
     }
    },
    "825a988043724380a053616c711e8401": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23b582e617554b7a874d246e48adf7f7",
      "placeholder": "",
      "style": "IPY_MODEL_c734ae403585464ba94cdf0f37b41ac4",
      "value": " 69.7k/69.7k [00:00&lt;00:00, 1.64MB/s]"
     }
    },
    "8b131288e18946a19cd649f6ca43d2a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "97c5e47863024e3b9a1fb13b9e8868d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98ba59932d084e008bd851cb96927063": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d87d89e25d1346a29fb19bbc9fcffa85",
      "placeholder": "",
      "style": "IPY_MODEL_ff202effd8964c98b8e2fc5c52264d68",
      "value": "Downloading ()lve/main/config.json: 100%"
     }
    },
    "9ff2ca6160cd4ad283657bceb6831691": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10bc8d5b2bf14502abf97c47a27ee2b7",
      "max": 69665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b131288e18946a19cd649f6ca43d2a9",
      "value": 69665
     }
    },
    "c734ae403585464ba94cdf0f37b41ac4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d09292f5afc045cba07318db5e171471": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d87d89e25d1346a29fb19bbc9fcffa85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "deb144cf25e74807b3adb4fd0fc6ba26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f1c630d0443b44c484db793403db5713",
       "IPY_MODEL_62305803798b4130899d2e93a3b1a5e2",
       "IPY_MODEL_48901010590e483da4289444ec48c40a"
      ],
      "layout": "IPY_MODEL_d09292f5afc045cba07318db5e171471"
     }
    },
    "e7d1c5825ecc466493d5da9633b635c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f06d20046f89484e9180562d0a115aee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f1c630d0443b44c484db793403db5713": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7d1c5825ecc466493d5da9633b635c8",
      "placeholder": "",
      "style": "IPY_MODEL_97c5e47863024e3b9a1fb13b9e8868d9",
      "value": "Downloading ()&quot;pytorch_model.bin&quot;;: 100%"
     }
    },
    "f615c90dfc314e0294b2e32b81f07b5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_98ba59932d084e008bd851cb96927063",
       "IPY_MODEL_9ff2ca6160cd4ad283657bceb6831691",
       "IPY_MODEL_825a988043724380a053616c711e8401"
      ],
      "layout": "IPY_MODEL_1b59827d64e84940b15ac6e47f5d2e05"
     }
    },
    "ff202effd8964c98b8e2fc5c52264d68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
