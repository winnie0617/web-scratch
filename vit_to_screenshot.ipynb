{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Quick_demo_of_HuggingFace_version_of_Vision_Transformer_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czHcRYWitu-6"
   },
   "source": [
    "Adapted from Quick demo: Vision Transformer (ViT) by Google Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:17:37 INFO:Logger set up\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')\n",
    "logger.info(\"Logger set up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Preprocess Data\n",
    "Mark all pixels that belongs to the bounding boxes of positive candidates as targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "09:17:39 INFO:PyTorch version 2.0.1 available.\n",
      "09:17:39 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:39 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web HTTP/1.1\" 200 5343\n",
      "09:17:39 DEBUG:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "09:17:39 DEBUG:https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/osunlp/Multimodal-Mind2Web/osunlp/Multimodal-Mind2Web.py HTTP/1.1\" 404 0\n",
      "09:17:39 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web HTTP/1.1\" 200 5343\n",
      "09:17:39 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:39 DEBUG:https://huggingface.co:443 \"HEAD /datasets/osunlp/Multimodal-Mind2Web/resolve/f27b6362acc6efe0e97289620307ca42cb177e5b/README.md HTTP/1.1\" 200 0\n",
      "09:17:39 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "09:17:40 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "09:17:40 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "09:17:40 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/tree/f27b6362acc6efe0e97289620307ca42cb177e5b/data?recursive=False&expand=False HTTP/1.1\" 200 12241\n",
      "09:17:40 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "09:17:40 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "09:17:40 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "09:17:40 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "09:17:40 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "09:17:40 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "09:17:40 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:40 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:40 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:40 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:40 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:40 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:40 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:40 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:40 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:40 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:40 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:40 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:40 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:41 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "09:17:41 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d10a939ed3402fbdd46a7df2a7f5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:17:41 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "09:17:41 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "09:17:42 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "09:17:42 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "09:17:42 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "09:17:42 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "09:17:42 DEBUG:https://huggingface.co:443 \"HEAD /datasets/osunlp/Multimodal-Mind2Web/resolve/f27b6362acc6efe0e97289620307ca42cb177e5b/dataset_infos.json HTTP/1.1\" 404 0\n",
      "09:17:42 DEBUG:Attempting to acquire lock 140139545269520 on /scr/wychow/.cache/huggingface/datasets/_scr_wychow_.cache_huggingface_datasets_osunlp___multimodal-mind2_web_default_0.0.0_f27b6362acc6efe0e97289620307ca42cb177e5b.lock\n",
      "09:17:42 DEBUG:Lock 140139545269520 acquired on /scr/wychow/.cache/huggingface/datasets/_scr_wychow_.cache_huggingface_datasets_osunlp___multimodal-mind2_web_default_0.0.0_f27b6362acc6efe0e97289620307ca42cb177e5b.lock\n",
      "09:17:42 DEBUG:open file: /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b/dataset_info.json\n",
      "09:17:42 DEBUG:Attempting to release lock 140139545269520 on /scr/wychow/.cache/huggingface/datasets/_scr_wychow_.cache_huggingface_datasets_osunlp___multimodal-mind2_web_default_0.0.0_f27b6362acc6efe0e97289620307ca42cb177e5b.lock\n",
      "09:17:42 DEBUG:Lock 140139545269520 released on /scr/wychow/.cache/huggingface/datasets/_scr_wychow_.cache_huggingface_datasets_osunlp___multimodal-mind2_web_default_0.0.0_f27b6362acc6efe0e97289620307ca42cb177e5b.lock\n",
      "09:17:42 DEBUG:Attempting to acquire lock 140139545584400 on /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b_builder.lock\n",
      "09:17:42 DEBUG:Lock 140139545584400 acquired on /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b_builder.lock\n",
      "09:17:42 DEBUG:open file: /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b/dataset_info.json\n",
      "09:17:42 DEBUG:Attempting to release lock 140139545584400 on /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b_builder.lock\n",
      "09:17:42 DEBUG:Lock 140139545584400 released on /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b_builder.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['action_uid', 'raw_html', 'cleaned_html', 'operation', 'pos_candidates', 'neg_candidates', 'website', 'domain', 'subdomain', 'annotation_id', 'confirmed_task', 'screenshot', 'action_reprs'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['action_uid', 'operation', 'pos_candidates', 'website', 'domain', 'subdomain', 'annotation_id', 'confirmed_task', 'screenshot', 'previous_actions'],\n",
      "    num_rows: 892\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "from datasets import load_dataset\n",
    "\n",
    "set_seed(123)\n",
    "# logger.info(f\"Use model {cfg.model.pretrained_model_name_or_path}\")\n",
    "# output_dir = HydraConfig.get().runtime.output_dir\n",
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(1000))\n",
    "# train_dataset = train_dataset.map(\n",
    "#     flatten_actions,\n",
    "#     batched=True,\n",
    "#     remove_columns=train_dataset.column_names, # remove all original columns?\n",
    "#     batch_size=10,\n",
    "#     num_proc=4,\n",
    "# )\n",
    "print(train_dataset)\n",
    "train_dataset = train_dataset.remove_columns([\"neg_candidates\", \"raw_html\", \"cleaned_html\"])\n",
    "\n",
    "# Add column for previous_actions\n",
    "previous_actions = []\n",
    "curr_actions = None\n",
    "num_actions = 0\n",
    "step = 0\n",
    "for i in range(len(train_dataset)):    \n",
    "    if step == num_actions:\n",
    "        step = 0\n",
    "        curr_actions = train_dataset[i][\"action_reprs\"]\n",
    "        num_actions = len(curr_actions)\n",
    "    previous_actions.append(curr_actions[:step]) \n",
    "    step += 1\n",
    "\n",
    "train_dataset = train_dataset.add_column(\"previous_actions\", previous_actions)\n",
    "\n",
    "# filter out those without pos_candidates\n",
    "train_dataset = train_dataset.filter(lambda example: len(example[\"pos_candidates\"]) == 1, num_proc=20) #TODO\n",
    "train_dataset = train_dataset.remove_columns('action_reprs')\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prompt and label\n",
    "The full prompt is:\n",
    "\n",
    "[patch embeddings] \\n Based on the webpage screenshot, try to complete the following task:\\n Task: [task] \\n Previous actions:\\n [actions] \\n Which image patch contains the element to interact with next?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_prompt_target(example):\n",
    "    \"\"\"\n",
    "    Use the bounding boxes of pos_candidates (as list of lists, [left, bottom, width, height]\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    for cand in example[\"pos_candidates\"]:\n",
    "        json_data = json.loads(cand)\n",
    "        attributes = json.loads(json_data['attributes'])\n",
    "        bounding_box_rect_str = attributes['bounding_box_rect']\n",
    "        boxes.append(list(map(float, bounding_box_rect_str.split(','))))\n",
    "\n",
    "    # NOTE: Don't prune, just include the whole webpage\n",
    "    seq_input = (\n",
    "        \"Based on the HTML webpage, try to complete the following task:\\n\"\n",
    "        f\"Task: {example['confirmed_task']}\\n\"\n",
    "        f\"Previous actions:\\n\"\n",
    "    )\n",
    "    # TODO: hard-coded\n",
    "    previous_k = 5\n",
    "    if len(example[\"previous_actions\"]) > 0:\n",
    "        for action in example[\"previous_actions\"][-previous_k:]:\n",
    "            seq_input += f\"{action}\\n\"\n",
    "    else:\n",
    "        seq_input += \"None\\n\"\n",
    "        \n",
    "    seq_input += (\n",
    "        \"What should be the element to interact with next?\"\n",
    "    )\n",
    "\n",
    "    example[\"question\"] = seq_input\n",
    "    example[\"boxes\"] = boxes\n",
    "\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f6862ccc62495aa7b7ff7e6b96dab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/892 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:17:46 DEBUG:open file: /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b/tmpgnehgny4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'screenshot': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x5429>,\n",
       " 'question': 'Based on the HTML webpage, try to complete the following task:\\nTask: rent a car in Brooklyn - Central, NY on from April 9 to April 15.\\nPrevious actions:\\n[heading]  CAR -> CLICK\\n[combobox]  Enter pick up city, airport name, or airport code. -> TYPE: Brooklyn Central\\nWhat should be the element to interact with next?',\n",
       " 'boxes': [[114.59375, 365.1875, 306.8125, 25.6875]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_remove = set(train_dataset.column_names)\n",
    "cols_to_remove.remove(\"screenshot\")\n",
    "train_dataset = train_dataset.map(\n",
    "    get_prompt_target,\n",
    "    batched=False,\n",
    "    remove_columns=list(cols_to_remove)\n",
    ")\n",
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultimodalAgent(PreTrainedModel):\n",
    "    def __init__(self, config, image_encoder, lm):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.supports_gradient_checkpointing = True\n",
    "        self.image_encoder = image_encoder\n",
    "        self.projector = nn.Linear(image_encoder.config.hidden_size, lm.config.hidden_size) \n",
    "        self.lm = lm\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask, labels=None):\n",
    "        # embed pixel_values with image_encoder\n",
    "        # h_image = self.image_encoder(flattened_patches, attention_mask_image).last_hidden_state\n",
    "        h_image = self.image_encoder(pixel_values, interpolate_pos_encoding=True).last_hidden_state\n",
    "        # linear layer to project hidden states to lm's input dimension\n",
    "        h_image = self.projector(h_image)\n",
    "        # look up token embedding for text\n",
    "        h_text = self.lm.model.embed_tokens(input_ids)\n",
    "        # concatenate image represenation with question\n",
    "        inputs_embeds = torch.cat([h_image, h_text], dim=1)\n",
    "        # also concat attention mask\n",
    "        # attention_mask = torch.cat([torch.ones(h_image.shape), attention_mask], dim=-1)\n",
    "        # TODO: need to add some sort of separator, like \\n?\n",
    "        return self.lm(inputs_embeds=inputs_embeds, output_hidden_states=True).hidden_states[-1] # Not passing attention mask, no need for now since batch size is 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:17:47 DEBUG:https://huggingface.co:443 \"HEAD /mistralai/Mistral-7B-v0.1/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "09:17:47 DEBUG:https://huggingface.co:443 \"HEAD /google/vit-base-patch16-224/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7b32c1485f420da7239b74b43a3491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cf8512d79f4af492de5bd824a72b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:18:00 DEBUG:https://huggingface.co:443 \"HEAD /mistralai/Mistral-7B-v0.1/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "09:18:02 DEBUG:https://huggingface.co:443 \"HEAD /mistralai/Mistral-7B-v0.1/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15378507776\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "import torch\n",
    "# from transformers import Pix2StructVisionModel, ViTImageProcessor, Pix2StructVisionConfig\n",
    "\n",
    "### Config for notebook\n",
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "config.return_dict = True\n",
    "config.use_cache = False\n",
    "config.low_cpu_mem_usage = True\n",
    "config.rope_theta = 10000.0\n",
    "config.attn_implementation = \"flash_attention_2\"\n",
    "###\n",
    "\n",
    "# TODO: Move config to somewhere else\n",
    "\n",
    "# image_encoder_config = Pix2StructVisionConfig.from_pretrained(\"google/pix2struct-base\")\n",
    "# TODO: try different hidden size?\n",
    "# image_encoder_config.seq_len = 27145\n",
    "# image_encoder_config.patch_size = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# image_encoder = Pix2StructVisionModel.from_pretrained(\"google/pix2struct-textcaps-base\", config=image_encoder_config, torch_dtype=torch.bfloat16)\n",
    "# image_encoder.to(device)\n",
    "\n",
    "image_encoder_path = \"google/vit-base-patch16-224\"\n",
    "image_encoder_config = AutoConfig.from_pretrained(image_encoder_path)\n",
    "image_encoder = AutoModel.from_pretrained(image_encoder_path, config=image_encoder_config)\n",
    "image_encoder.to(device)\n",
    "\n",
    "lm_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "lm = AutoModelForCausalLM.from_pretrained(lm_path, config=config, torch_dtype=torch.bfloat16)\n",
    "lm.to(device)\n",
    "\n",
    "model = MultimodalAgent(config, image_encoder, lm)\n",
    "model.to(device)\n",
    "print(torch.cuda.memory_allocated())\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(lm_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token # should be ok for casual LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:18:02 DEBUG:https://huggingface.co:443 \"HEAD /google/vit-base-patch16-224/resolve/main/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "09:18:02 DEBUG:https://huggingface.co:443 \"HEAD /google/vit-base-patch16-224/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "import math\n",
    "max_patches = 2000\n",
    "# max_patches = 200\n",
    "patch_height, patch_width = 16, 16\n",
    "processor = AutoImageProcessor.from_pretrained(image_encoder_path) # TODO: define this somewhere else\n",
    "\n",
    "def preprocess_training_examples(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenize and map char index of the target to token index\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(examples[\"question\"] + \" [ACT]\")\n",
    "    inputs[\"labels\"] = examples[\"boxes\"]\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def preprocess_training_examples_with_tokenizer(tokenizer):\n",
    "    return lambda examples: preprocess_training_examples(examples, tokenizer)\n",
    "\n",
    "def preprocess_image(example):\n",
    "    \"\"\" \n",
    "    Aspect ratio preserving, fixed size patches \n",
    "    reference: https://github.com/huggingface/transformers/blob/main/src/transformers/models/pix2struct/image_processing_pix2struct.py\n",
    "    \"\"\"\n",
    "    \n",
    "    image_width, image_height = example[\"screenshot\"][0].size\n",
    "    # maximize scale s.t.\n",
    "    scale = math.sqrt(max_patches * (patch_height / image_height) * (patch_width / image_width))\n",
    "    num_feasible_rows = max(min(math.floor(scale * image_height / patch_height), max_patches), 1)\n",
    "    num_feasible_cols = max(min(math.floor(scale * image_width / patch_width), max_patches), 1)\n",
    "    resized_height = max(num_feasible_rows * patch_height, 1)\n",
    "    resized_width = max(num_feasible_cols * patch_width, 1)\n",
    "    \n",
    "    processor.size = {\"height\":resized_height, \"width\":resized_width}\n",
    "    inputs = processor(images=example[\"screenshot\"], return_tensors=\"pt\")\n",
    "    # example[\"screenshot\"] = inputs[\"flattened_patches\"]\n",
    "    example[\"screenshot\"] = inputs[\"pixel_values\"]\n",
    "    all_scaled_boxes = []\n",
    "    x_scale = image_width / resized_width\n",
    "    y_scale = image_height / resized_height\n",
    "    for boxes in example[\"labels\"]:\n",
    "        scaled_boxes = []\n",
    "        for box in boxes:\n",
    "            scaled_boxes.append([box[0]/x_scale, box[1]/y_scale, box[2]/x_scale, box[3]/y_scale])\n",
    "        all_scaled_boxes.append(scaled_boxes)\n",
    "    example[\"labels\"] = all_scaled_boxes\n",
    "    # example[\"attention_mask_image\"] = inputs[\"attention_mask\"]\n",
    "    return example\n",
    "    # return {\"pixel_values\": processor(images=example[\"screenshot\"], return_tensors=\"pt\").pixel_values} #[1, 3, 224, 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72c0ef36f2048268ce299e05ab25223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/892 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:18:02 DEBUG:open file: /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b/tmp7md2d6az\n",
      "09:18:04 DEBUG:open file: /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b/tmpizz2e3e_\n",
      "09:18:04 DEBUG:open file: /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b/tmpku9sf08u\n",
      "09:18:04 INFO:Use device gpu\n",
      "09:18:04 INFO:Training data size 802\n",
      "09:18:04 INFO:Eval data size 90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'screenshot': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x5429 at 0x7F74A445EF90>, 'input_ids': [1, 17158, 356, 272, 13987, 4686, 3005, 28725, 1464, 298, 4160, 272, 2296, 3638, 28747, 13, 4818, 28747, 7358, 264, 1253, 297, 21491, 387, 7993, 28725, 11800, 356, 477, 3999, 28705, 28774, 298, 3999, 28705, 28740, 28782, 28723, 13, 28284, 6768, 28747, 13, 5364, 13, 3195, 1023, 347, 272, 2442, 298, 14113, 395, 1679, 28804, 733, 7637, 28793], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [[283.1875, 220.390625, 93.59375, 33.0]]}\n",
      "{'screenshot': tensor([[[-0.9922, -0.9922, -0.9922,  ..., -0.9922, -0.9922, -0.9922],\n",
      "         [-0.9922, -0.9922, -0.9922,  ..., -0.9922, -0.9922, -0.9922],\n",
      "         [-0.9922, -0.9922, -0.9922,  ..., -0.9922, -0.9922, -0.9922],\n",
      "         ...,\n",
      "         [ 0.7961,  0.7961,  0.7961,  ...,  0.7961,  0.7961,  0.7961],\n",
      "         [ 0.7961,  0.7961,  0.7961,  ...,  0.7961,  0.7961,  0.7961],\n",
      "         [ 0.7961,  0.7961,  0.7961,  ...,  0.7961,  0.7961,  0.7961]],\n",
      "\n",
      "        [[-0.7333, -0.7333, -0.7333,  ..., -0.7333, -0.7333, -0.7333],\n",
      "         [-0.7333, -0.7333, -0.7333,  ..., -0.7333, -0.7333, -0.7333],\n",
      "         [-0.7333, -0.7333, -0.7333,  ..., -0.7333, -0.7333, -0.7333],\n",
      "         ...,\n",
      "         [ 0.8353,  0.8353,  0.8353,  ...,  0.8353,  0.8353,  0.8353],\n",
      "         [ 0.8353,  0.8353,  0.8353,  ...,  0.8353,  0.8353,  0.8353],\n",
      "         [ 0.8353,  0.8353,  0.8353,  ...,  0.8353,  0.8353,  0.8353]],\n",
      "\n",
      "        [[-0.4902, -0.4902, -0.4902,  ..., -0.4902, -0.4902, -0.4902],\n",
      "         [-0.4902, -0.4902, -0.4902,  ..., -0.4902, -0.4902, -0.4902],\n",
      "         [-0.4902, -0.4902, -0.4902,  ..., -0.4902, -0.4902, -0.4902],\n",
      "         ...,\n",
      "         [ 0.8667,  0.8667,  0.8667,  ...,  0.8667,  0.8667,  0.8667],\n",
      "         [ 0.8667,  0.8667,  0.8667,  ...,  0.8667,  0.8667,  0.8667],\n",
      "         [ 0.8667,  0.8667,  0.8667,  ...,  0.8667,  0.8667,  0.8667]]]), 'input_ids': [1, 17158, 356, 272, 13987, 4686, 3005, 28725, 1464, 298, 4160, 272, 2296, 3638, 28747, 13, 4818, 28747, 11147, 354, 22447, 2632, 477, 2984, 28721, 628, 325, 28743, 16655, 28743, 28731, 298, 1450, 2726, 325, 28828, 24743, 609, 13, 28284, 6768, 28747, 13, 5364, 13, 3195, 1023, 347, 272, 2442, 298, 14113, 395, 1679, 28804, 733, 7637, 28793], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [[121.275, 150.3959057551178, 43.65625, 8.800308999613751]]}\n"
     ]
    }
   ],
   "source": [
    "cols = train_dataset.column_names\n",
    "cols.remove(\"screenshot\")\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_training_examples_with_tokenizer(tokenizer),\n",
    "    remove_columns=cols,\n",
    "    )\n",
    "# processor = ViTImageProcessor(size={\"height\": 5429, \"width\": 1280})\n",
    "# train_dataset[\"pixel_values\"] = train_dataset.map(preprocess_image, remove_columns=train_dataset.column_names,\n",
    "#     )\n",
    "\n",
    "# train_dataset.set_format(\"pt\", columns=[\"input_ids\", \"attention_mask\", \"label\"], output_all_columns=True)\n",
    "print(train_dataset[0])\n",
    "train_dataset.set_transform(preprocess_image, output_all_columns=True) # process images on the fly\n",
    "# split the train_dataset into train and validation\n",
    "dataset = train_dataset.train_test_split(test_size=0.1) \n",
    "train_dataset, eval_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "print(train_dataset[0])\n",
    "logger.info(f\"Use device {'gpu' if torch.cuda.is_available() else 'cpu'}\")\n",
    "# logger.info(f\"Use batch size {cfg.train.batch_size}\")\n",
    "logger.info(f\"Training data size {len(train_dataset)}\")\n",
    "logger.info(f\"Eval data size {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 45,277,184 || all params: 7,376,548,352 || trainable%: 0.6137990539670769\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.pooler.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.pooler.dense.lora_B.default.weight\n",
      "base_model.model.projector.lora_A.default.weight\n",
      "base_model.model.projector.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.lm_head.lora_A.default.weight\n",
      "base_model.model.lm.lm_head.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_int8_training\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    # task_type=TaskType.CAUSAL_LM, # task type is not necessary, but this is needed to get the label\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.05,\n",
    "    target_modules = \"all-linear\"\n",
    ")\n",
    "\n",
    "# model.lm.enable_input_require_grads()\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "#         'lora_config': lora_config,\n",
    "#         'learning_rate': cfg.train.learning_rate,\n",
    "#         'num_train_epochs': cfg.train.epoch,\n",
    "#         'gradient_accumulation_steps': cfg.train.gradient_accumulation_steps,\n",
    "#         'per_device_train_batch_size': cfg.train.batch_size,\n",
    "#         'per_device_eval_batch_size': cfg.eval.eval_batch_size,\n",
    "#         'eval_accumulation_steps': cfg.eval.eval_accumulation_steps,\n",
    "#         'gradient_checkpointing': True,\n",
    "# }\n",
    "config = {\n",
    "        'lora_config': lora_config,\n",
    "        'learning_rate': 3e-5,\n",
    "        'num_train_epochs': 1,\n",
    "        'gradient_accumulation_steps': 8,\n",
    "        'per_device_train_batch_size': 1,\n",
    "        'per_device_eval_batch_size': 1,\n",
    "        'eval_accumulation_steps': 32,\n",
    "        'gradient_checkpointing': True,\n",
    "}\n",
    "\n",
    "# class CustomTrainer(Trainer):\n",
    "    \n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "#         hidden_states = model(inputs[\"input_ids\"], inputs[\"attention_mask\"], output_hidden_states=True).hidden_states[-1]\n",
    "#         # compute cosine simularity between last token and every token before\n",
    "#         temperature = 0.1 # TODO: hard coded\n",
    "#         sim = torch.nn.functional.cosine_similarity(hidden_states[:,:-3,:], hidden_states[:,-1:,:], dim=2) # Last 3 tokens are \"[\", \"ACT\", \"]\"\n",
    "#         target_idx = inputs[\"labels\"]\n",
    "\n",
    "#         loss = torch.nn.functional.cross_entropy(sim / temperature, target_idx)\n",
    "\n",
    "#         if return_outputs:\n",
    "#             # instead of returning all hidden_states which would be too much memory,\n",
    "#             # return the similarity scores as \"logits\"\n",
    "#             # but different than sim because sin only calculates for \n",
    "#             # scores = torch.nn.functional.cosine_similarity(hidden_states[:,:-1,:], hidden_states[:,-1:,:], dim=2)\n",
    "#             return loss, {\"similarity\": sim}\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# def boxes_to_patch_idx_multitarget(box, num_cols):\n",
    "#     \"\"\" box is a tensor. Returns a list \"\"\"\n",
    "#     # pos_idxs = set()\n",
    "#     l, b, w, h = box[0], box[1], box[2], box[3]\n",
    "#     # unscaled 2d idx -> scaled 2d idx\n",
    "#     x1, x2 = l//downscale_factor, (l+w)//downscale_factor\n",
    "#     y1, y2 = b//downscale_factor, (b+h)//downscale_factor\n",
    "#     # scaled 2d idx -> patch 2d idx\n",
    "#     x1, x2 = math.floor(x1/16), math.ceil(x2/16)\n",
    "#     y1, y2 = math.floor(y1/16), math.ceil(y2/16)\n",
    "#     # 2d -> 1d\n",
    "#     return [num_cols*r + c for c in range(x1, x2) for r in range(y1, y2)]\n",
    "\n",
    "def boxes_to_patch_idx(box, num_cols):\n",
    "    \"\"\" returns the patch closest to the center of the element \"\"\"\n",
    "    # pos_idxs = set()\n",
    "    l, b, w, h = box[0], box[1], box[2], box[3]\n",
    "    # scaled 2d coordinate -> patch 2d coordinate\n",
    "    x1, x2 = l/patch_width, (l+w)/patch_width\n",
    "    y1, y2 = b/patch_height, (b+h)/patch_height\n",
    "    # patch 2d coordinate -> 1d idx\n",
    "    c = math.floor((x1+x2)/2)\n",
    "    r = math.floor((y1+y2)/2)\n",
    "    # if x2 - x1 >= 2: # element at least contains 1 whole patch\n",
    "    # else: # element within 2 patches\n",
    "    return [num_cols*r + c]\n",
    "\n",
    "def patch_idx_to_click(patch_idx, num_cols):\n",
    "    \"\"\" (x, y), default to clicking the centre of the patch\"\"\"\n",
    "    r, c = patch_idx // num_cols, patch_idx % num_cols\n",
    "    return patch_width * (c+0.5), patch_height * (r+0.5)\n",
    "    \n",
    "class MultimodalTrainer(Trainer):\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \n",
    "        # hidden_states = model(flattened_patches=inputs[\"flattened_patches\"], attention_mask_image=inputs[\"attention_mask_image\"], input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        hidden_states = model(pixel_values=inputs[\"pixel_values\"], input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        # compute cosine simularity between last token and every token before\n",
    "        temperature = 0.1 # TODO: hard coded\n",
    "        # sim = torch.nn.functional.cosine_similarity(hidden_states[:,:-3,:], hidden_states[:,-1:,:], dim=2) # Last 3 tokens are \"[\", \"ACT\", \"]\"\n",
    "        num_cols = inputs[\"pixel_values\"].shape[-1] // patch_width\n",
    "        num_rows = inputs[\"pixel_values\"].shape[-2] // patch_width\n",
    "        num_patches = num_cols * num_rows\n",
    "        sim = torch.nn.functional.cosine_similarity(hidden_states[:,1:num_patches+1,:], hidden_states[:,-1:,:], dim=2) # Last 3 tokens are \"[\", \"ACT\", \"]\"\n",
    "        pos_idxs = set()\n",
    "        \n",
    "        for box in inputs[\"labels\"][0]: # TODO: only for batch size \n",
    "            pos_idxs.update(boxes_to_patch_idx(box, num_cols))\n",
    "        # +1 because first idx is CLS\n",
    "        # target_idx = torch.tensor([idx + 1 for idx in pos_idxs]).to(device)\n",
    "        \n",
    "        target_idx = torch.tensor(list(pos_idxs)).to(device)\n",
    "        \n",
    "        # print(\"box\", inputs[\"labels\"][0])\n",
    "        # print(\"click coordinate\", patch_idx_to_click(target_idx, num_cols))\n",
    "        loss = torch.nn.functional.cross_entropy(sim / temperature, target_idx) # TODO: use BCE for multitarget?\n",
    "        # print(loss)\n",
    "        # print(\"prediction\", torch.argmax(sim).item(), \"actual\", target_idx.item())\n",
    "        # print(torch.max(sim), sim[0,target_idx])\n",
    "        if return_outputs:\n",
    "            # instead of returning all hidden_states which would be too much memory,\n",
    "            # return the similarity scores as \"logits\"\n",
    "            # but different than sim because sin only calculates for \n",
    "            # scores = torch.nn.functional.cosine_similarity(hidden_states[:,:-1,:], hidden_states[:,-1:,:], dim=2)\n",
    "            return loss, {\"sim\":sim, \"target_idx\":target_idx}\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def custom_collate(data):\n",
    "    # flattened_patches = torch.stack([d['screenshot'] for d in data])\n",
    "    pixel_values = torch.stack([d['screenshot'] for d in data])\n",
    "    # input_ids = torch.stack([d['input_ids'] for d in data])\n",
    "    input_ids = torch.tensor([d['input_ids'] for d in data]) # set_transform resets set_format :(\n",
    "    attention_mask = torch.tensor([d['attention_mask'] for d in data])\n",
    "    # attention_mask_image = torch.stack([d['attention_mask_image'] for d in data])\n",
    "    labels = torch.tensor([d['labels'] for d in data]) # todo: only uses first positive\n",
    "    return { \n",
    "        'pixel_values': pixel_values,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        # 'attention_mask_image': attention_mask_image,\n",
    "        'labels': labels,\n",
    "    }\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    sims, target_idxs = pred.predictions[0], pred.predictions[1]\n",
    "    accuracy = []\n",
    "    # Need to use a for loop because sequence length is different for each input\n",
    "    preds = sims.argmax(axis=1)\n",
    "    # print(np.max(sims, axis=1), [sims[r][i] for r, i in zip(range(len(target_idxs)), target_idxs)])\n",
    "    accuracy = preds == target_idxs # TODO: use information from bounding box to get more metrics\n",
    "    # bounding box stored in pred.label_ids\n",
    "    return {\n",
    "        'accuracy': np.array(accuracy).mean(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:18:05 WARNING:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Using auto half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 802\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 45,277,184\n",
      "/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 46:21, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>7.281100</td>\n",
       "      <td>6.771319</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>6.245400</td>\n",
       "      <td>6.330454</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>6.034000</td>\n",
       "      <td>6.216006</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>5.902500</td>\n",
       "      <td>6.105346</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>5.713900</td>\n",
       "      <td>6.065022</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>5.590100</td>\n",
       "      <td>6.065434</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=6.116548614501953, metrics={'train_runtime': 2807.0115, 'train_samples_per_second': 0.571, 'train_steps_per_second': 0.036, 'total_flos': 6742814184603648.0, 'train_loss': 6.116548614501953, 'epoch': 2.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     overwrite_output_dir=True,\n",
    "#     optim=\"adamw_torch_fused\",\n",
    "#     bf16=True,  # Use BF16 for flash attention\n",
    "#     # evlaution\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=cfg.eval.eval_steps,\n",
    "#     include_inputs_for_metrics=True,\n",
    "#     # logging strategies\n",
    "#     logging_dir=f\"{output_dir}/logs\",\n",
    "#     logging_strategy=\"steps\",\n",
    "#     logging_steps=10,\n",
    "#     save_strategy=\"no\",\n",
    "#     **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    "# ) # TODO: move train arguments to config\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    overwrite_output_dir=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    bf16=True,  # Use BF16 for flash attention\n",
    "    # evlaution\n",
    "    label_names=[\"labels\"], # so that trainer will call compute_loss\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=8,\n",
    "    include_inputs_for_metrics=True,\n",
    "    log_level=\"info\",\n",
    "    # logging strategies\n",
    "    logging_dir=f\"output/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=8,\n",
    "    save_strategy=\"no\",\n",
    "    remove_unused_columns=False,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ") # TODO: move train arguments to config\n",
    "trainer = MultimodalTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=custom_collate,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pix2Struct, reference: https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Invalid key: 997 is out of bounds for size 802",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoProcessor, Pix2StructVisionModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# processor = AutoProcessor.from_pretrained(\"google/pix2struct-base\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model = Pix2StructVisionModel.from_pretrained(\"google/pix2struct-base\")\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m997\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:2800\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:2784\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2783\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2784\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2785\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2786\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2787\u001b[0m )\n\u001b[1;32m   2788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/datasets/formatting/formatting.py:583\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n\u001b[0;32m--> 583\u001b[0m     \u001b[43m_check_valid_index_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# Query the main table\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/datasets/formatting/formatting.py:526\u001b[0m, in \u001b[0;36m_check_valid_index_key\u001b[0;34m(key, size)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m size):\n\u001b[0;32m--> 526\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of bounds for size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n",
      "\u001b[0;31mIndexError\u001b[0m: Invalid key: 997 is out of bounds for size 802"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Pix2StructVisionModel\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"google/pix2struct-base\")\n",
    "# model = Pix2StructVisionModel.from_pretrained(\"google/pix2struct-base\")\n",
    "train_dataset[997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor, Pix2StructVisionModel\n",
    "\n",
    "\n",
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(10))\n",
    "image = train_dataset[3][\"screenshot\"]\n",
    "\n",
    "text = \"A picture of\"\n",
    "\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-textcaps-base\")\n",
    "processor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n",
    "# processor = Pix2StructImageProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n",
    "\n",
    "# image only\n",
    "inputs = processor(images=train_dataset[\"screenshot\"], text=text, return_tensors=\"pt\")\n",
    "print(inputs.keys())\n",
    "predictions = model.generate(**inputs)\n",
    "print(processor.decode(predictions[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pix2Struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pix2StructImageProcessor, Pix2StructVisionModel, Pix2StructConfig, Pix2StructForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(10))\n",
    "image = train_dataset[3][\"screenshot\"]\n",
    "\n",
    "# TODO: Move config to somewhere else\n",
    "image_encoder_path = \"google/pix2struct-textcaps-base\"\n",
    "image_encoder_config = Pix2StructConfig.from_pretrained(image_encoder_path)\n",
    "# TODO: try different hidden size?\n",
    "max_patches = 2000\n",
    "patch_size = 16\n",
    "# image_encoder_config.vision_config.seq_len = max_patches\n",
    "# image_encoder_config.vision_config.patch_size = patch_size\n",
    "print(image_encoder_config)\n",
    "\n",
    "image_encoder = Pix2StructForConditionalGeneration.from_pretrained(image_encoder_path, config=image_encoder_config).encoder\n",
    "print(image_encoder)\n",
    "image_encoder.to(device)\n",
    "\n",
    "processor = Pix2StructImageProcessor.from_pretrained(image_encoder_path) # TODO: define this somewhere else\n",
    "processor.max_patches = max_patches\n",
    "processor.patch_size = {\"height\":patch_size, \"width\":patch_size}\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "print(image_encoder(**inputs))\n",
    "print(torch.cuda.memory_summary())\n",
    "# 2000 -> 7G\n",
    "# 3000 -> 14G\n",
    "# 4000 -> 25G\n",
    "# 5000 -> 37G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoImageProcessor, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(10))\n",
    "image = train_dataset[3][\"screenshot\"]\n",
    "\n",
    "# TODO: Move config to somewhere else\n",
    "image_encoder_path = \"google/vit-base-patch16-224\"\n",
    "image_encoder_config = AutoConfig.from_pretrained(image_encoder_path)\n",
    "# TODO: try different hidden size?\n",
    "# print(image_encoder_config)\n",
    "\n",
    "image_encoder = AutoModel.from_pretrained(image_encoder_path, config=image_encoder_config)\n",
    "# print(image_encoder)\n",
    "image_encoder.to(device)\n",
    "\n",
    "downscale_factor = 4\n",
    "processor = AutoImageProcessor.from_pretrained(image_encoder_path) # TODO: define this somewhere else\n",
    "processor.size = {\"height\":5429//downscale_factor, \"width\":1280//downscale_factor}\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "print(inputs.pixel_values.shape)\n",
    "plt.figure(figsize=(12, 40))\n",
    "plt.imshow(inputs.pixel_values.cpu()[0].permute((1,2,0)))\n",
    "plt.show()\n",
    "h = image_encoder(inputs[\"pixel_values\"], interpolate_pos_encoding=True).last_hidden_state\n",
    "# print(torch.cuda.memory_summary())\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match downscaled image patch index to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[3][\"pos_candidates\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match target index to patch index\n",
    "\n",
    "\n",
    "bounding_box_rect is in the format of (left, bottom, width, height), so pixel_values[:,bottom:bottom+height,left:left+width] should be marked as positive\n",
    "\n",
    "unscaled index 2d -> scaled index 2d -> patch index 2d -> patch index 1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortest width / height:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\")\n",
    "cands = train_dataset[\"pos_candidates\"]\n",
    "shortest = 100\n",
    "widths = []\n",
    "heights = []\n",
    "import json\n",
    "for cand_list in cands:\n",
    "    for cand in cand_list:\n",
    "        json_data = json.loads(cand)\n",
    "        attributes = json.loads(json_data['attributes'])\n",
    "        bounding_box_rect_str = attributes['bounding_box_rect']\n",
    "        lbwh = tuple(map(float, bounding_box_rect_str.split(',')))\n",
    "        widths.append(lbwh[2])\n",
    "        heights.append(lbwh[3])\n",
    "        # if lbwh[2] <= 0 or lbwh[3] <= 0:\n",
    "        #     print(cand_list)\n",
    "        #     print(shortest)\n",
    "\n",
    "        # shortest = min(shortest, lbwh[2], lbwh[3])\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "# plt.hist(widths, bins=100)\n",
    "# plt.show()\n",
    "heights = np.array(heights)\n",
    "plt.hist(heights[heights < 200], bins=100)\n",
    "plt.axvline(x=32, color='r', linestyle='--')\n",
    "plt.title(\"Pos candidates height\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "import torch\n",
    "sample = train_dataset[3]\n",
    "print(sample[\"pos_candidates\"])\n",
    "\n",
    "image = sample[\"screenshot\"]\n",
    "print(image.size)\n",
    "processor = ViTImageProcessor(size={\"height\": 5429, \"width\": 1280})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values # [1, 3, 224, 224]\n",
    "print(pixel_values.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure()\n",
    "plt.imshow(np.transpose(pixel_values[0,:,410:410+46,96:96+106].cpu(), (1,2,0)))\n",
    "\n",
    "\n",
    "processor2 = ViTImageProcessor(size={\"height\": 5429//2, \"width\": 1280//2})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inputs2 = processor2(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values2 = inputs2.pixel_values # [1, 3, 224, 224]\n",
    "print(pixel_values2.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure()\n",
    "plt.imshow(np.transpose(pixel_values2[0,:,410//2:(410+46)//2,96//2:(96+106)//2].cpu(), (1,2,0)))\n",
    "\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.imshow(np.transpose(pixel_values[0].cpu(), (1,2,0)))\n",
    "# for i in range(0, 1000, 100):\n",
    "#     plt.figure()\n",
    "#     plt.imshow(np.transpose(pixel_values[0,:,i:i+160,i:i+160].cpu(), (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def boxes_to_patch_idx_multitarget(box, num_cols):\n",
    "    \"\"\" box is a tensor. Returns a list \"\"\"\n",
    "    # pos_idxs = set()\n",
    "    l, b, w, h = box[0], box[1], box[2], box[3]\n",
    "    # unscaled 2d idx -> scaled 2d idx\n",
    "    x1, x2 = l//2, (l+w)//2\n",
    "    y1, y2 = b//2, (b+h)//2\n",
    "    # scaled 2d idx -> patch 2d idx\n",
    "    x1, x2 = math.floor(x1/16), math.ceil(x2/16)\n",
    "    y1, y2 = math.floor(y1/16), math.ceil(y2/16)\n",
    "    # 2d -> 1d\n",
    "    return [num_cols*r + c for c in range(x1, x2) for r in range(y1, y2)]\n",
    "\n",
    "def boxes_to_patch_idx(box, num_cols):\n",
    "    \"\"\" returns the patch closest to the center of the element \"\"\"\n",
    "    # pos_idxs = set()\n",
    "    l, b, w, h = box[0], box[1], box[2], box[3]\n",
    "    # unscaled 2d idx -> scaled 2d idx\n",
    "    x1, x2 = l//2, (l+w)//2\n",
    "    y1, y2 = b//2, (b+h)//2\n",
    "    # scaled 2d idx -> patch 2d idx\n",
    "    x1, x2 = x1/16, x2/16\n",
    "    y1, y2 = y1/16, y2/16\n",
    "    # 2d -> 1d\n",
    "    c = math.floor((x1+x2)/2)\n",
    "    r = math.floor((y1+y2)/2)\n",
    "    # if x2 - x1 >= 2: # element at least contains 1 whole patch\n",
    "    # else: # element within 2 patches\n",
    "    return num_cols*r + c\n",
    "\n",
    "# for i in range(16):\n",
    "#     print([i*16+j for j in range(16)])\n",
    "print(boxes_to_patch_idx_multitarget([96,410.390625,106,46], 640//16))\n",
    "boxes_to_patch_idx([96,410.390625,106,46], 640//16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:46:23 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web HTTP/1.1\" 200 5343\n",
      "07:46:23 DEBUG:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "07:46:24 DEBUG:https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/osunlp/Multimodal-Mind2Web/osunlp/Multimodal-Mind2Web.py HTTP/1.1\" 404 0\n",
      "07:46:24 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web HTTP/1.1\" 200 5343\n",
      "07:46:24 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "07:46:24 DEBUG:https://huggingface.co:443 \"HEAD /datasets/osunlp/Multimodal-Mind2Web/resolve/f27b6362acc6efe0e97289620307ca42cb177e5b/README.md HTTP/1.1\" 200 0\n",
      "07:46:24 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "07:46:24 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "07:46:24 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "07:46:24 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "07:46:24 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "07:46:25 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "07:46:25 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "07:46:25 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "07:46:25 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf0811463564686a573769deb7445e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:46:25 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "07:46:25 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "07:46:25 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "07:46:25 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "07:46:26 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "07:46:26 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "07:46:26 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "07:46:26 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "07:46:26 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "07:46:26 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "07:46:26 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "07:46:27 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "07:46:27 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "07:46:27 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "07:46:27 DEBUG:https://huggingface.co:443 \"HEAD /datasets/osunlp/Multimodal-Mind2Web/resolve/f27b6362acc6efe0e97289620307ca42cb177e5b/dataset_infos.json HTTP/1.1\" 404 0\n",
      "07:46:27 DEBUG:Attempting to acquire lock 140120319182032 on /scr/wychow/.cache/huggingface/datasets/_scr_wychow_.cache_huggingface_datasets_osunlp___multimodal-mind2_web_default_0.0.0_f27b6362acc6efe0e97289620307ca42cb177e5b.lock\n",
      "07:46:27 DEBUG:Lock 140120319182032 acquired on /scr/wychow/.cache/huggingface/datasets/_scr_wychow_.cache_huggingface_datasets_osunlp___multimodal-mind2_web_default_0.0.0_f27b6362acc6efe0e97289620307ca42cb177e5b.lock\n",
      "07:46:27 DEBUG:open file: /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b/dataset_info.json\n",
      "07:46:27 DEBUG:Attempting to release lock 140120319182032 on /scr/wychow/.cache/huggingface/datasets/_scr_wychow_.cache_huggingface_datasets_osunlp___multimodal-mind2_web_default_0.0.0_f27b6362acc6efe0e97289620307ca42cb177e5b.lock\n",
      "07:46:27 DEBUG:Lock 140120319182032 released on /scr/wychow/.cache/huggingface/datasets/_scr_wychow_.cache_huggingface_datasets_osunlp___multimodal-mind2_web_default_0.0.0_f27b6362acc6efe0e97289620307ca42cb177e5b.lock\n",
      "07:46:27 DEBUG:Attempting to acquire lock 140139546118160 on /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b_builder.lock\n",
      "07:46:27 DEBUG:Lock 140139546118160 acquired on /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b_builder.lock\n",
      "07:46:27 DEBUG:open file: /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b/dataset_info.json\n",
      "07:46:27 DEBUG:Attempting to release lock 140139546118160 on /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b_builder.lock\n",
      "07:46:27 DEBUG:Lock 140139546118160 released on /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b_builder.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b644db96ed9b47ccaf25ed0ed3c33957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'previous_actions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m cols_to_remove \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m     27\u001b[0m cols_to_remove\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscreenshot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_prompt_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcols_to_remove\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# filter out those without pos_candidates\u001b[39;00m\n\u001b[1;32m     35\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m example: \u001b[38;5;28mlen\u001b[39m(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_candidates\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m) \u001b[38;5;66;03m#TODO\u001b[39;00m\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3093\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3088\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3089\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3090\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3091\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3092\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3093\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3094\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3095\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3446\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3444\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3445\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3446\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3448\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3349\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3348\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3349\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3351\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3352\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3353\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mget_prompt_target\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# TODO: hard-coded\u001b[39;00m\n\u001b[1;32m     20\u001b[0m previous_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprevious_actions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_actions\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39mprevious_k:]:\n\u001b[1;32m     23\u001b[0m         seq_input \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/datasets/formatting/formatting.py:270\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 270\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[1;32m    272\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'previous_actions'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoImageProcessor, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(20))\n",
    "cols_to_remove = set(train_dataset.column_names)\n",
    "cols_to_remove.remove(\"screenshot\")\n",
    "train_dataset = train_dataset.map(\n",
    "    get_prompt_target,\n",
    "    batched=False,\n",
    "    remove_columns=list(cols_to_remove)\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"neg_candidates\", \"raw_html\", \"cleaned_html\"])\n",
    "\n",
    "# Add column for previous_actions\n",
    "previous_actions = []\n",
    "curr_actions = None\n",
    "num_actions = 0\n",
    "step = 0\n",
    "for i in range(len(train_dataset)):    \n",
    "    if step == num_actions:\n",
    "        step = 0\n",
    "        curr_actions = train_dataset[i][\"action_reprs\"]\n",
    "        num_actions = len(curr_actions)\n",
    "    previous_actions.append(curr_actions[:step]) \n",
    "    step += 1\n",
    "\n",
    "train_dataset = train_dataset.add_column(\"previous_actions\", previous_actions)\n",
    "\n",
    "# filter out those without pos_candidates\n",
    "train_dataset = train_dataset.filter(lambda example: len(example[\"pos_candidates\"]) == 1, num_proc=20) #TODO\n",
    "train_dataset = train_dataset.remove_columns('action_reprs')\n",
    "\n",
    "def preprocess_image(example):\n",
    "    \"\"\" \n",
    "    Aspect ratio preserving, fixed size patches \n",
    "    reference: https://github.com/huggingface/transformers/blob/main/src/transformers/models/pix2struct/image_processing_pix2struct.py\n",
    "    \"\"\"\n",
    "    max_patches = 2000\n",
    "    image_width, image_height = example[\"screenshot\"][0].size\n",
    "    # maximize scale s.t.\n",
    "    scale = math.sqrt(max_patches * (patch_height / image_height) * (patch_width / image_width))\n",
    "    num_feasible_rows = max(min(math.floor(scale * image_height / patch_height), max_patches), 1)\n",
    "    num_feasible_cols = max(min(math.floor(scale * image_width / patch_width), max_patches), 1)\n",
    "    resized_height = max(num_feasible_rows * patch_height, 1)\n",
    "    resized_width = max(num_feasible_cols * patch_width, 1)\n",
    "    \n",
    "    processor.size = {\"height\":resized_height, \"width\":resized_width}\n",
    "    inputs = processor(images=example[\"screenshot\"], return_tensors=\"pt\")\n",
    "    # example[\"screenshot\"] = inputs[\"flattened_patches\"]\n",
    "\n",
    "    all_scaled_boxes = []\n",
    "    x_scale = image_width / resized_width\n",
    "    y_scale = image_height / resized_height\n",
    "    for boxes in example[\"labels\"]:\n",
    "        scaled_boxes = []\n",
    "        for box in boxes:\n",
    "            scaled_boxes.append([box[0]/x_scale, box[1]/y_scale, box[2]/x_scale, box[3]/y_scale])\n",
    "        all_scaled_boxes.append(scaled_boxes)\n",
    "    example[\"labels\"] = all_scaled_boxes\n",
    "    plt.figure(figsize=(12, 40))\n",
    "    plt.imshow(inputs.pixel_values.cpu()[0].permute((1,2,0)))\n",
    "    plt.show()\n",
    "    # example[\"attention_mask_image\"] = inputs[\"attention_mask\"]\n",
    "    # return {\"pixel_values\": processor(images=example[\"screenshot\"], return_tensors=\"pt\").pixel_values} #[1, 3, 224, 224]\n",
    "\n",
    "preprocess_image(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03c0d0f4f4de45dcb6364473147606b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10bc8d5b2bf14502abf97c47a27ee2b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b59827d64e84940b15ac6e47f5d2e05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23b582e617554b7a874d246e48adf7f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b536d7f4e7b40b38db4c583fcf01a2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2d5cfaa9cbd3499ea81b87e4a2b3ef52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48901010590e483da4289444ec48c40a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d5cfaa9cbd3499ea81b87e4a2b3ef52",
      "placeholder": "​",
      "style": "IPY_MODEL_f06d20046f89484e9180562d0a115aee",
      "value": " 346M/346M [00:03&lt;00:00, 93.3MB/s]"
     }
    },
    "62305803798b4130899d2e93a3b1a5e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03c0d0f4f4de45dcb6364473147606b0",
      "max": 346351599,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b536d7f4e7b40b38db4c583fcf01a2f",
      "value": 346351599
     }
    },
    "825a988043724380a053616c711e8401": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23b582e617554b7a874d246e48adf7f7",
      "placeholder": "​",
      "style": "IPY_MODEL_c734ae403585464ba94cdf0f37b41ac4",
      "value": " 69.7k/69.7k [00:00&lt;00:00, 1.64MB/s]"
     }
    },
    "8b131288e18946a19cd649f6ca43d2a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "97c5e47863024e3b9a1fb13b9e8868d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98ba59932d084e008bd851cb96927063": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d87d89e25d1346a29fb19bbc9fcffa85",
      "placeholder": "​",
      "style": "IPY_MODEL_ff202effd8964c98b8e2fc5c52264d68",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "9ff2ca6160cd4ad283657bceb6831691": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10bc8d5b2bf14502abf97c47a27ee2b7",
      "max": 69665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b131288e18946a19cd649f6ca43d2a9",
      "value": 69665
     }
    },
    "c734ae403585464ba94cdf0f37b41ac4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d09292f5afc045cba07318db5e171471": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d87d89e25d1346a29fb19bbc9fcffa85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "deb144cf25e74807b3adb4fd0fc6ba26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f1c630d0443b44c484db793403db5713",
       "IPY_MODEL_62305803798b4130899d2e93a3b1a5e2",
       "IPY_MODEL_48901010590e483da4289444ec48c40a"
      ],
      "layout": "IPY_MODEL_d09292f5afc045cba07318db5e171471"
     }
    },
    "e7d1c5825ecc466493d5da9633b635c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f06d20046f89484e9180562d0a115aee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f1c630d0443b44c484db793403db5713": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7d1c5825ecc466493d5da9633b635c8",
      "placeholder": "​",
      "style": "IPY_MODEL_97c5e47863024e3b9a1fb13b9e8868d9",
      "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
     }
    },
    "f615c90dfc314e0294b2e32b81f07b5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_98ba59932d084e008bd851cb96927063",
       "IPY_MODEL_9ff2ca6160cd4ad283657bceb6831691",
       "IPY_MODEL_825a988043724380a053616c711e8401"
      ],
      "layout": "IPY_MODEL_1b59827d64e84940b15ac6e47f5d2e05"
     }
    },
    "ff202effd8964c98b8e2fc5c52264d68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
