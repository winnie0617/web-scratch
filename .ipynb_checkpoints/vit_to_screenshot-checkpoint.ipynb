{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Quick_demo_of_HuggingFace_version_of_Vision_Transformer_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czHcRYWitu-6"
   },
   "source": [
    "Adapted from Quick demo: Vision Transformer (ViT) by Google Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:54:52 INFO:Logger set up\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')\n",
    "logger.info(\"Logger set up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Preprocess Data\n",
    "Mark all pixels that belongs to the bounding boxes of positive candidates as targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "02:54:54 INFO:PyTorch version 2.0.1 available.\n",
      "02:54:54 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:54 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web HTTP/1.1\" 200 5343\n",
      "02:54:54 DEBUG:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "02:54:54 DEBUG:https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/osunlp/Multimodal-Mind2Web/osunlp/Multimodal-Mind2Web.py HTTP/1.1\" 404 0\n",
      "02:54:54 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web HTTP/1.1\" 200 5343\n",
      "02:54:54 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:55 DEBUG:https://huggingface.co:443 \"HEAD /datasets/osunlp/Multimodal-Mind2Web/resolve/f27b6362acc6efe0e97289620307ca42cb177e5b/README.md HTTP/1.1\" 200 0\n",
      "02:54:55 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "02:54:55 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "02:54:55 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "02:54:55 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/tree/f27b6362acc6efe0e97289620307ca42cb177e5b/data?recursive=False&expand=False HTTP/1.1\" 200 12241\n",
      "02:54:55 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "02:54:55 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "02:54:55 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "02:54:55 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "02:54:55 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "02:54:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:56 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9b7a01665b42f68c9078612bec6f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:54:56 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"GET /api/datasets/osunlp/Multimodal-Mind2Web/revision/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 5343\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "02:54:56 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "02:54:57 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "02:54:57 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "02:54:57 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "02:54:57 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "02:54:57 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "02:54:57 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "02:54:57 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 97\n",
      "02:54:57 DEBUG:https://huggingface.co:443 \"POST /api/datasets/osunlp/Multimodal-Mind2Web/paths-info/f27b6362acc6efe0e97289620307ca42cb177e5b HTTP/1.1\" 200 283\n",
      "02:54:57 DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "02:54:58 DEBUG:https://huggingface.co:443 \"HEAD /datasets/osunlp/Multimodal-Mind2Web/resolve/f27b6362acc6efe0e97289620307ca42cb177e5b/dataset_infos.json HTTP/1.1\" 404 0\n",
      "02:54:58 DEBUG:Attempting to acquire lock 140277193828816 on /scr/wychow/.cache/huggingface/datasets/_scr_wychow_.cache_huggingface_datasets_osunlp___multimodal-mind2_web_default_0.0.0_f27b6362acc6efe0e97289620307ca42cb177e5b.lock\n",
      "02:54:58 DEBUG:Lock 140277193828816 acquired on /scr/wychow/.cache/huggingface/datasets/_scr_wychow_.cache_huggingface_datasets_osunlp___multimodal-mind2_web_default_0.0.0_f27b6362acc6efe0e97289620307ca42cb177e5b.lock\n",
      "02:54:58 DEBUG:open file: /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b/dataset_info.json\n",
      "02:54:58 DEBUG:Attempting to release lock 140277193828816 on /scr/wychow/.cache/huggingface/datasets/_scr_wychow_.cache_huggingface_datasets_osunlp___multimodal-mind2_web_default_0.0.0_f27b6362acc6efe0e97289620307ca42cb177e5b.lock\n",
      "02:54:58 DEBUG:Lock 140277193828816 released on /scr/wychow/.cache/huggingface/datasets/_scr_wychow_.cache_huggingface_datasets_osunlp___multimodal-mind2_web_default_0.0.0_f27b6362acc6efe0e97289620307ca42cb177e5b.lock\n",
      "02:54:58 DEBUG:Attempting to acquire lock 140276979185168 on /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b_builder.lock\n",
      "02:54:58 DEBUG:Lock 140276979185168 acquired on /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b_builder.lock\n",
      "02:54:58 DEBUG:open file: /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b/dataset_info.json\n",
      "02:54:58 DEBUG:Attempting to release lock 140276979185168 on /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b_builder.lock\n",
      "02:54:58 DEBUG:Lock 140276979185168 released on /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b_builder.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['action_uid', 'raw_html', 'cleaned_html', 'operation', 'pos_candidates', 'neg_candidates', 'website', 'domain', 'subdomain', 'annotation_id', 'confirmed_task', 'screenshot', 'action_reprs'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['action_uid', 'operation', 'pos_candidates', 'website', 'domain', 'subdomain', 'annotation_id', 'confirmed_task', 'screenshot', 'previous_actions'],\n",
      "    num_rows: 892\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "from datasets import load_dataset\n",
    "\n",
    "set_seed(123)\n",
    "# logger.info(f\"Use model {cfg.model.pretrained_model_name_or_path}\")\n",
    "# output_dir = HydraConfig.get().runtime.output_dir\n",
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(1000))\n",
    "# train_dataset = train_dataset.map(\n",
    "#     flatten_actions,\n",
    "#     batched=True,\n",
    "#     remove_columns=train_dataset.column_names, # remove all original columns?\n",
    "#     batch_size=10,\n",
    "#     num_proc=4,\n",
    "# )\n",
    "print(train_dataset)\n",
    "train_dataset = train_dataset.remove_columns([\"neg_candidates\", \"raw_html\", \"cleaned_html\"])\n",
    "\n",
    "# Add column for previous_actions\n",
    "previous_actions = []\n",
    "curr_actions = None\n",
    "num_actions = 0\n",
    "step = 0\n",
    "for i in range(len(train_dataset)):    \n",
    "    if step == num_actions:\n",
    "        step = 0\n",
    "        curr_actions = train_dataset[i][\"action_reprs\"]\n",
    "        num_actions = len(curr_actions)\n",
    "    previous_actions.append(curr_actions[:step]) \n",
    "    step += 1\n",
    "\n",
    "train_dataset = train_dataset.add_column(\"previous_actions\", previous_actions)\n",
    "\n",
    "# filter out those without pos_candidates\n",
    "train_dataset = train_dataset.filter(lambda example: len(example[\"pos_candidates\"]) == 1, num_proc=20) #TODO\n",
    "train_dataset = train_dataset.remove_columns('action_reprs')\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prompt and label\n",
    "The full prompt is:\n",
    "\n",
    "[patch embeddings] \\n Based on the webpage screenshot, try to complete the following task:\\n Task: [task] \\n Previous actions:\\n [actions] \\n Which image patch contains the element to interact with next?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_prompt_target(example):\n",
    "    \"\"\"\n",
    "    Use the bounding boxes of pos_candidates (as list of lists, [left, bottom, width, height]\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    for cand in example[\"pos_candidates\"]:\n",
    "        json_data = json.loads(cand)\n",
    "        attributes = json.loads(json_data['attributes'])\n",
    "        bounding_box_rect_str = attributes['bounding_box_rect']\n",
    "        boxes.append(list(map(float, bounding_box_rect_str.split(','))))\n",
    "\n",
    "    # NOTE: Don't prune, just include the whole webpage\n",
    "    seq_input = (\n",
    "        \"Based on the HTML webpage, try to complete the following task:\\n\"\n",
    "        f\"Task: {example['confirmed_task']}\\n\"\n",
    "        f\"Previous actions:\\n\"\n",
    "    )\n",
    "    # TODO: hard-coded\n",
    "    previous_k = 5\n",
    "    if len(example[\"previous_actions\"]) > 0:\n",
    "        for action in example[\"previous_actions\"][-previous_k:]:\n",
    "            seq_input += f\"{action}\\n\"\n",
    "    else:\n",
    "        seq_input += \"None\\n\"\n",
    "        \n",
    "    seq_input += (\n",
    "        \"What should be the element to interact with next?\"\n",
    "    )\n",
    "\n",
    "    example[\"question\"] = seq_input\n",
    "    example[\"boxes\"] = boxes\n",
    "\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063afb3fb04342eb98edf8bb7bc9faa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/892 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:55:01 DEBUG:open file: /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b/tmpx5aoovv6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'screenshot': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x5429>,\n",
       " 'question': 'Based on the HTML webpage, try to complete the following task:\\nTask: rent a car in Brooklyn - Central, NY on from April 9 to April 15.\\nPrevious actions:\\n[heading]  CAR -> CLICK\\n[combobox]  Enter pick up city, airport name, or airport code. -> TYPE: Brooklyn Central\\nWhat should be the element to interact with next?',\n",
       " 'boxes': [[114.59375, 365.1875, 306.8125, 25.6875]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_remove = set(train_dataset.column_names)\n",
    "cols_to_remove.remove(\"screenshot\")\n",
    "train_dataset = train_dataset.map(\n",
    "    get_prompt_target,\n",
    "    batched=False,\n",
    "    remove_columns=list(cols_to_remove)\n",
    ")\n",
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultimodalAgent(PreTrainedModel):\n",
    "    def __init__(self, config, image_encoder, lm):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.supports_gradient_checkpointing = True\n",
    "        self.image_encoder = image_encoder\n",
    "        self.projector = nn.Linear(image_encoder.config.hidden_size, lm.config.hidden_size) \n",
    "        self.lm = lm\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask, labels=None):\n",
    "        # embed pixel_values with image_encoder\n",
    "        # h_image = self.image_encoder(flattened_patches, attention_mask_image).last_hidden_state\n",
    "        h_image = self.image_encoder(pixel_values, interpolate_pos_encoding=True).last_hidden_state\n",
    "        # linear layer to project hidden states to lm's input dimension\n",
    "        h_image = self.projector(h_image)\n",
    "        # look up token embedding for text\n",
    "        h_text = self.lm.model.embed_tokens(input_ids)\n",
    "        # concatenate image represenation with question\n",
    "        inputs_embeds = torch.cat([h_image, h_text], dim=1)\n",
    "        # also concat attention mask\n",
    "        # attention_mask = torch.cat([torch.ones(h_image.shape), attention_mask], dim=-1)\n",
    "        # TODO: need to add some sort of separator, like \\n?\n",
    "        return self.lm(inputs_embeds=inputs_embeds, output_hidden_states=True).hidden_states[-1] # Not passing attention mask, no need for now since batch size is 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:55:03 DEBUG:https://huggingface.co:443 \"HEAD /mistralai/Mistral-7B-v0.1/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "02:55:03 DEBUG:https://huggingface.co:443 \"HEAD /google/vit-base-patch16-224/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c767fb54014ae4ba46bcf5c309c1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f833a2fa93649c9adc4f18ad1995d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:55:12 DEBUG:https://huggingface.co:443 \"HEAD /mistralai/Mistral-7B-v0.1/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "02:55:14 DEBUG:https://huggingface.co:443 \"HEAD /mistralai/Mistral-7B-v0.1/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15378507776\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "import torch\n",
    "# from transformers import Pix2StructVisionModel, ViTImageProcessor, Pix2StructVisionConfig\n",
    "\n",
    "### Config for notebook\n",
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "config.return_dict = True\n",
    "config.use_cache = False\n",
    "config.low_cpu_mem_usage = True\n",
    "config.rope_theta = 10000.0\n",
    "config.attn_implementation = \"flash_attention_2\"\n",
    "###\n",
    "\n",
    "# TODO: Move config to somewhere else\n",
    "\n",
    "# image_encoder_config = Pix2StructVisionConfig.from_pretrained(\"google/pix2struct-base\")\n",
    "# TODO: try different hidden size?\n",
    "# image_encoder_config.seq_len = 27145\n",
    "# image_encoder_config.patch_size = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# image_encoder = Pix2StructVisionModel.from_pretrained(\"google/pix2struct-textcaps-base\", config=image_encoder_config, torch_dtype=torch.bfloat16)\n",
    "# image_encoder.to(device)\n",
    "\n",
    "image_encoder_path = \"google/vit-base-patch16-224\"\n",
    "image_encoder_config = AutoConfig.from_pretrained(image_encoder_path)\n",
    "image_encoder = AutoModel.from_pretrained(image_encoder_path, config=image_encoder_config)\n",
    "image_encoder.to(device)\n",
    "\n",
    "lm_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "lm = AutoModelForCausalLM.from_pretrained(lm_path, config=config, torch_dtype=torch.bfloat16)\n",
    "lm.to(device)\n",
    "\n",
    "model = MultimodalAgent(config, image_encoder, lm)\n",
    "model.to(device)\n",
    "print(torch.cuda.memory_allocated())\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(lm_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token # should be ok for casual LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:55:14 DEBUG:https://huggingface.co:443 \"HEAD /google/vit-base-patch16-224/resolve/main/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "02:55:14 DEBUG:https://huggingface.co:443 \"HEAD /google/vit-base-patch16-224/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "import math\n",
    "max_patches = 2000\n",
    "# max_patches = 200\n",
    "patch_height, patch_width = 16, 16\n",
    "processor = AutoImageProcessor.from_pretrained(image_encoder_path) # TODO: define this somewhere else\n",
    "\n",
    "def preprocess_training_examples(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenize and map char index of the target to token index\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(examples[\"question\"] + \" [ACT]\")\n",
    "    inputs[\"labels\"] = examples[\"boxes\"]\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def preprocess_training_examples_with_tokenizer(tokenizer):\n",
    "    return lambda examples: preprocess_training_examples(examples, tokenizer)\n",
    "\n",
    "def preprocess_image(example):\n",
    "    \"\"\" \n",
    "    Aspect ratio preserving, fixed size patches \n",
    "    reference: https://github.com/huggingface/transformers/blob/main/src/transformers/models/pix2struct/image_processing_pix2struct.py\n",
    "    \"\"\"\n",
    "    \n",
    "    image_width, image_height = example[\"screenshot\"][0].size\n",
    "    # maximize scale s.t.\n",
    "    scale = math.sqrt(max_patches * (patch_height / image_height) * (patch_width / image_width))\n",
    "    num_feasible_rows = max(min(math.floor(scale * image_height / patch_height), max_patches), 1)\n",
    "    num_feasible_cols = max(min(math.floor(scale * image_width / patch_width), max_patches), 1)\n",
    "    resized_height = max(num_feasible_rows * patch_height, 1)\n",
    "    resized_width = max(num_feasible_cols * patch_width, 1)\n",
    "    \n",
    "    processor.size = {\"height\":resized_height, \"width\":resized_width}\n",
    "    inputs = processor(images=example[\"screenshot\"], return_tensors=\"pt\")\n",
    "    # example[\"screenshot\"] = inputs[\"flattened_patches\"]\n",
    "    example[\"screenshot\"] = inputs[\"pixel_values\"]\n",
    "    all_scaled_boxes = []\n",
    "    x_scale = image_width / resized_width\n",
    "    y_scale = image_height / resized_height\n",
    "    for boxes in example[\"labels\"]:\n",
    "        scaled_boxes = []\n",
    "        for box in boxes:\n",
    "            scaled_boxes.append([box[0]/x_scale, box[1]/y_scale, box[2]/x_scale, box[3]/y_scale])\n",
    "        all_scaled_boxes.append(scaled_boxes)\n",
    "    example[\"labels\"] = all_scaled_boxes\n",
    "    # example[\"attention_mask_image\"] = inputs[\"attention_mask\"]\n",
    "    return example\n",
    "    # return {\"pixel_values\": processor(images=example[\"screenshot\"], return_tensors=\"pt\").pixel_values} #[1, 3, 224, 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0db5e7eeba4406ba7e1208ea3c5770d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/892 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:55:14 DEBUG:open file: /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b/tmp7n5tfm1v\n",
      "02:55:15 DEBUG:open file: /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b/tmp67uiu604\n",
      "02:55:15 DEBUG:open file: /scr/wychow/.cache/huggingface/datasets/osunlp___multimodal-mind2_web/default/0.0.0/f27b6362acc6efe0e97289620307ca42cb177e5b/tmprks_l0i2\n",
      "02:55:16 INFO:Use device gpu\n",
      "02:55:16 INFO:Training data size 802\n",
      "02:55:16 INFO:Eval data size 90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'screenshot': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x5429 at 0x7F94AC6DA350>, 'input_ids': [1, 17158, 356, 272, 13987, 4686, 3005, 28725, 1464, 298, 4160, 272, 2296, 3638, 28747, 13, 4818, 28747, 7358, 264, 1253, 297, 21491, 387, 7993, 28725, 11800, 356, 477, 3999, 28705, 28774, 298, 3999, 28705, 28740, 28782, 28723, 13, 28284, 6768, 28747, 13, 5364, 13, 3195, 1023, 347, 272, 2442, 298, 14113, 395, 1679, 28804, 733, 7637, 28793], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [[283.1875, 220.390625, 93.59375, 33.0]]}\n",
      "{'screenshot': tensor([[[-0.9922, -0.9922, -0.9922,  ..., -0.9922, -0.9922, -0.9922],\n",
      "         [-0.9922, -0.9922, -0.9922,  ..., -0.9922, -0.9922, -0.9922],\n",
      "         [-0.9922, -0.9922, -0.9922,  ..., -0.9922, -0.9922, -0.9922],\n",
      "         ...,\n",
      "         [ 0.7961,  0.7961,  0.7961,  ...,  0.7961,  0.7961,  0.7961],\n",
      "         [ 0.7961,  0.7961,  0.7961,  ...,  0.7961,  0.7961,  0.7961],\n",
      "         [ 0.7961,  0.7961,  0.7961,  ...,  0.7961,  0.7961,  0.7961]],\n",
      "\n",
      "        [[-0.7333, -0.7333, -0.7333,  ..., -0.7333, -0.7333, -0.7333],\n",
      "         [-0.7333, -0.7333, -0.7333,  ..., -0.7333, -0.7333, -0.7333],\n",
      "         [-0.7333, -0.7333, -0.7333,  ..., -0.7333, -0.7333, -0.7333],\n",
      "         ...,\n",
      "         [ 0.8353,  0.8353,  0.8353,  ...,  0.8353,  0.8353,  0.8353],\n",
      "         [ 0.8353,  0.8353,  0.8353,  ...,  0.8353,  0.8353,  0.8353],\n",
      "         [ 0.8353,  0.8353,  0.8353,  ...,  0.8353,  0.8353,  0.8353]],\n",
      "\n",
      "        [[-0.4902, -0.4902, -0.4902,  ..., -0.4902, -0.4902, -0.4902],\n",
      "         [-0.4902, -0.4902, -0.4902,  ..., -0.4902, -0.4902, -0.4902],\n",
      "         [-0.4902, -0.4902, -0.4902,  ..., -0.4902, -0.4902, -0.4902],\n",
      "         ...,\n",
      "         [ 0.8667,  0.8667,  0.8667,  ...,  0.8667,  0.8667,  0.8667],\n",
      "         [ 0.8667,  0.8667,  0.8667,  ...,  0.8667,  0.8667,  0.8667],\n",
      "         [ 0.8667,  0.8667,  0.8667,  ...,  0.8667,  0.8667,  0.8667]]]), 'input_ids': [1, 17158, 356, 272, 13987, 4686, 3005, 28725, 1464, 298, 4160, 272, 2296, 3638, 28747, 13, 4818, 28747, 11147, 354, 22447, 2632, 477, 2984, 28721, 628, 325, 28743, 16655, 28743, 28731, 298, 1450, 2726, 325, 28828, 24743, 609, 13, 28284, 6768, 28747, 13, 5364, 13, 3195, 1023, 347, 272, 2442, 298, 14113, 395, 1679, 28804, 733, 7637, 28793], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [[121.275, 150.3959057551178, 43.65625, 8.800308999613751]]}\n"
     ]
    }
   ],
   "source": [
    "cols = train_dataset.column_names\n",
    "cols.remove(\"screenshot\")\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_training_examples_with_tokenizer(tokenizer),\n",
    "    remove_columns=cols,\n",
    "    )\n",
    "# processor = ViTImageProcessor(size={\"height\": 5429, \"width\": 1280})\n",
    "# train_dataset[\"pixel_values\"] = train_dataset.map(preprocess_image, remove_columns=train_dataset.column_names,\n",
    "#     )\n",
    "\n",
    "# train_dataset.set_format(\"pt\", columns=[\"input_ids\", \"attention_mask\", \"label\"], output_all_columns=True)\n",
    "print(train_dataset[0])\n",
    "train_dataset.set_transform(preprocess_image, output_all_columns=True) # process images on the fly\n",
    "# split the train_dataset into train and validation\n",
    "dataset = train_dataset.train_test_split(test_size=0.1) \n",
    "train_dataset, eval_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "print(train_dataset[0])\n",
    "logger.info(f\"Use device {'gpu' if torch.cuda.is_available() else 'cpu'}\")\n",
    "# logger.info(f\"Use batch size {cfg.train.batch_size}\")\n",
    "logger.info(f\"Training data size {len(train_dataset)}\")\n",
    "logger.info(f\"Eval data size {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 45,277,184 || all params: 7,376,548,352 || trainable%: 0.6137990539670769\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.0.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.1.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.2.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.3.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.4.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.5.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.6.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.7.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.8.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.9.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.10.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.intermediate.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.intermediate.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.output.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.encoder.layer.11.output.dense.lora_B.default.weight\n",
      "base_model.model.image_encoder.pooler.dense.lora_A.default.weight\n",
      "base_model.model.image_encoder.pooler.dense.lora_B.default.weight\n",
      "base_model.model.projector.lora_A.default.weight\n",
      "base_model.model.projector.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.1.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.2.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.3.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.4.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.5.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.6.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.7.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.8.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.9.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.10.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.11.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.12.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.13.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.14.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.15.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.16.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.17.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.18.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.19.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.20.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.21.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.22.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.23.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.24.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.25.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.26.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.27.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.28.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.29.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.30.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.lm.model.layers.31.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.lm.lm_head.lora_A.default.weight\n",
      "base_model.model.lm.lm_head.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_int8_training\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    # task_type=TaskType.CAUSAL_LM, # task type is not necessary, but this is needed to get the label\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.05,\n",
    "    target_modules = \"all-linear\"\n",
    ")\n",
    "\n",
    "# model.lm.enable_input_require_grads()\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "#         'lora_config': lora_config,\n",
    "#         'learning_rate': cfg.train.learning_rate,\n",
    "#         'num_train_epochs': cfg.train.epoch,\n",
    "#         'gradient_accumulation_steps': cfg.train.gradient_accumulation_steps,\n",
    "#         'per_device_train_batch_size': cfg.train.batch_size,\n",
    "#         'per_device_eval_batch_size': cfg.eval.eval_batch_size,\n",
    "#         'eval_accumulation_steps': cfg.eval.eval_accumulation_steps,\n",
    "#         'gradient_checkpointing': True,\n",
    "# }\n",
    "config = {\n",
    "        'lora_config': lora_config,\n",
    "        'learning_rate': 3e-5,\n",
    "        'num_train_epochs': 1,\n",
    "        'gradient_accumulation_steps': 8,\n",
    "        'per_device_train_batch_size': 1,\n",
    "        'per_device_eval_batch_size': 1,\n",
    "        'eval_accumulation_steps': 32,\n",
    "        'gradient_checkpointing': True,\n",
    "}\n",
    "\n",
    "# class CustomTrainer(Trainer):\n",
    "    \n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "#         hidden_states = model(inputs[\"input_ids\"], inputs[\"attention_mask\"], output_hidden_states=True).hidden_states[-1]\n",
    "#         # compute cosine simularity between last token and every token before\n",
    "#         temperature = 0.1 # TODO: hard coded\n",
    "#         sim = torch.nn.functional.cosine_similarity(hidden_states[:,:-3,:], hidden_states[:,-1:,:], dim=2) # Last 3 tokens are \"[\", \"ACT\", \"]\"\n",
    "#         target_idx = inputs[\"labels\"]\n",
    "\n",
    "#         loss = torch.nn.functional.cross_entropy(sim / temperature, target_idx)\n",
    "\n",
    "#         if return_outputs:\n",
    "#             # instead of returning all hidden_states which would be too much memory,\n",
    "#             # return the similarity scores as \"logits\"\n",
    "#             # but different than sim because sin only calculates for \n",
    "#             # scores = torch.nn.functional.cosine_similarity(hidden_states[:,:-1,:], hidden_states[:,-1:,:], dim=2)\n",
    "#             return loss, {\"similarity\": sim}\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# def boxes_to_patch_idx_multitarget(box, num_cols):\n",
    "#     \"\"\" box is a tensor. Returns a list \"\"\"\n",
    "#     # pos_idxs = set()\n",
    "#     l, b, w, h = box[0], box[1], box[2], box[3]\n",
    "#     # unscaled 2d idx -> scaled 2d idx\n",
    "#     x1, x2 = l//downscale_factor, (l+w)//downscale_factor\n",
    "#     y1, y2 = b//downscale_factor, (b+h)//downscale_factor\n",
    "#     # scaled 2d idx -> patch 2d idx\n",
    "#     x1, x2 = math.floor(x1/16), math.ceil(x2/16)\n",
    "#     y1, y2 = math.floor(y1/16), math.ceil(y2/16)\n",
    "#     # 2d -> 1d\n",
    "#     return [num_cols*r + c for c in range(x1, x2) for r in range(y1, y2)]\n",
    "\n",
    "def boxes_to_patch_idx(box, num_cols):\n",
    "    \"\"\" returns the patch closest to the center of the element \"\"\"\n",
    "    # pos_idxs = set()\n",
    "    l, b, w, h = box[0], box[1], box[2], box[3]\n",
    "    # scaled 2d coordinate -> patch 2d coordinate\n",
    "    x1, x2 = l/patch_width, (l+w)/patch_width\n",
    "    y1, y2 = b/patch_height, (b+h)/patch_height\n",
    "    # patch 2d coordinate -> 1d idx\n",
    "    c = math.floor((x1+x2)/2)\n",
    "    r = math.floor((y1+y2)/2)\n",
    "    # if x2 - x1 >= 2: # element at least contains 1 whole patch\n",
    "    # else: # element within 2 patches\n",
    "    return [num_cols*r + c]\n",
    "\n",
    "def patch_idx_to_click(patch_idx, num_cols):\n",
    "    \"\"\" (x, y), default to clicking the centre of the patch\"\"\"\n",
    "    r, c = patch_idx // num_cols, patch_idx % num_cols\n",
    "    return patch_width * (c+0.5), patch_height * (r+0.5)\n",
    "    \n",
    "class MultimodalTrainer(Trainer):\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \n",
    "        # hidden_states = model(flattened_patches=inputs[\"flattened_patches\"], attention_mask_image=inputs[\"attention_mask_image\"], input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        hidden_states = model(pixel_values=inputs[\"pixel_values\"], input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        # compute cosine simularity between last token and every token before\n",
    "        temperature = 0.1 # TODO: hard coded\n",
    "        # sim = torch.nn.functional.cosine_similarity(hidden_states[:,:-3,:], hidden_states[:,-1:,:], dim=2) # Last 3 tokens are \"[\", \"ACT\", \"]\"\n",
    "        num_cols = inputs[\"pixel_values\"].shape[-1] // patch_width\n",
    "        num_rows = inputs[\"pixel_values\"].shape[-2] // patch_width\n",
    "        num_patches = num_cols * num_rows\n",
    "        sim = torch.nn.functional.cosine_similarity(hidden_states[:,1:num_patches+1,:], hidden_states[:,-1:,:], dim=2) # Last 3 tokens are \"[\", \"ACT\", \"]\"\n",
    "        pos_idxs = set()\n",
    "        \n",
    "        for box in inputs[\"labels\"][0]: # TODO: only for batch size \n",
    "            pos_idxs.update(boxes_to_patch_idx(box, num_cols))\n",
    "        # +1 because first idx is CLS\n",
    "        # target_idx = torch.tensor([idx + 1 for idx in pos_idxs]).to(device)\n",
    "        \n",
    "        target_idx = torch.tensor(list(pos_idxs)).to(device)\n",
    "        \n",
    "        # print(\"box\", inputs[\"labels\"][0])\n",
    "        # print(\"click coordinate\", patch_idx_to_click(target_idx, num_cols))\n",
    "        loss = torch.nn.functional.cross_entropy(sim / temperature, target_idx) # TODO: use BCE for multitarget?\n",
    "        # print(loss)\n",
    "        # print(\"prediction\", torch.argmax(sim).item(), \"actual\", target_idx.item())\n",
    "        # print(torch.max(sim), sim[0,target_idx])\n",
    "        if return_outputs:\n",
    "            # instead of returning all hidden_states which would be too much memory,\n",
    "            # return the similarity scores as \"logits\"\n",
    "            # but different than sim because sin only calculates for \n",
    "            # scores = torch.nn.functional.cosine_similarity(hidden_states[:,:-1,:], hidden_states[:,-1:,:], dim=2)\n",
    "            return loss, {\"sim\":sim, \"target_idx\":target_idx}\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def custom_collate(data):\n",
    "    # flattened_patches = torch.stack([d['screenshot'] for d in data])\n",
    "    pixel_values = torch.stack([d['screenshot'] for d in data])\n",
    "    # input_ids = torch.stack([d['input_ids'] for d in data])\n",
    "    input_ids = torch.tensor([d['input_ids'] for d in data]) # set_transform resets set_format :(\n",
    "    attention_mask = torch.tensor([d['attention_mask'] for d in data])\n",
    "    # attention_mask_image = torch.stack([d['attention_mask_image'] for d in data])\n",
    "    labels = torch.tensor([d['labels'] for d in data]) # todo: only uses first positive\n",
    "    return { \n",
    "        'pixel_values': pixel_values,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        # 'attention_mask_image': attention_mask_image,\n",
    "        'labels': labels,\n",
    "    }\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    sims, target_idxs = pred.predictions[0], pred.predictions[1]\n",
    "    accuracy = []\n",
    "    # Need to use a for loop because sequence length is different for each input\n",
    "    preds = sims.argmax(axis=1)\n",
    "    print(preds)\n",
    "    print(target_idxs)\n",
    "    accuracy = preds == target_idxs # TODO: use information from bounding box to get more metrics\n",
    "    # bounding box stored in pred.label_ids\n",
    "    return {\n",
    "        'accuracy': np.array(accuracy).mean(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:55:17 WARNING:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Using auto half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 802\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 45,277,184\n",
      "/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 30:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>7.528000</td>\n",
       "      <td>7.290866</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>7.094600</td>\n",
       "      <td>6.914609</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>6.581600</td>\n",
       "      <td>6.477030</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>6.524000</td>\n",
       "      <td>6.376817</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.007300</td>\n",
       "      <td>6.511876</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>6.171800</td>\n",
       "      <td>6.313993</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>6.019600</td>\n",
       "      <td>6.255442</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>5.887100</td>\n",
       "      <td>6.292933</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>5.838900</td>\n",
       "      <td>6.282590</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>5.912300</td>\n",
       "      <td>6.199437</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>6.144200</td>\n",
       "      <td>6.173467</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>5.824500</td>\n",
       "      <td>6.172019</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0  227  366   90   69  383  572    6  201  319  137   83 1238  105\n",
      "  523 1175   57   91  149  276   13   57   63 1605  129  340   76  108\n",
      "  373   98  179   58  597   35   10   19    8  224  862  157  296   14\n",
      "   48  294   73  327  183   46 1083   48 1352    8   19  554   45    9\n",
      "  617   58   63  214   55  373   42   16  204   10   64  374 1192 1518\n",
      "   31  171 1719   43    6  298   99  143  352   31   70  244   80   30\n",
      "   95  758  334  236    3 1518]\n",
      "[ 165  430  581   57 1771 1145 1264  298   87  430   27  702  535  155\n",
      "   86  533   57  197   33  501   58  379  168 1299   90  126  108  703\n",
      "  155  853  179  133  214   52  183  194  515  580   21  407   94  146\n",
      "  111  216   27  648  226  297  112   40  149  315  358  639  396 1763\n",
      "   36   20  167  275  122  247  822  203  736  198  118  125 1306  598\n",
      "  104  152   82  129  403  437  193  151  149  417  217  158  177  180\n",
      " 1141  107  581  351  621  598]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 116   20  334   97   69  383    5    6   88  176  497  169  516    4\n",
      "   82   36   57  268  149  632   13   88    8  446  129   70  117   44\n",
      "  155   66   99   61   71   47   10   19    8   44  293   40  296   14\n",
      "   48  294   14  327  183   50 1083   25  113    8   19  459  122  275\n",
      " 1921   60    8   99   55  234   46   16  202   10   64   27  525   53\n",
      "  266   13 1584   60    8  352   44  155   85   29   99  244   31   44\n",
      "   95   27  263  236    3   53]\n",
      "[ 165  430  581   57 1771 1145 1264  298   87  430   27  702  535  155\n",
      "   86  533   57  197   33  501   58  379  168 1299   90  126  108  703\n",
      "  155  853  179  133  214   52  183  194  515  580   21  407   94  146\n",
      "  111  216   27  648  226  297  112   40  149  315  358  639  396 1763\n",
      "   36   20  167  275  122  247  822  203  736  198  118  125 1306  598\n",
      "  104  152   82  129  403  437  193  151  149  417  217  158  177  180\n",
      " 1141  107  581  351  621  598]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 116   20   29   96   69  383    5    6  138  176  497  168  516    4\n",
      "   81    6   64    5  149  121   13   57    8  446  129   70  117   44\n",
      "  151   66  166   25   70   47    9   19    8   84  293   40  296   14\n",
      "   48  215   14  327  183   50 1082   25   95   91   19   93   61  275\n",
      "    1   79    8  129   55  234   46    4  204    9   64  178  436   48\n",
      "   19   13   32   60    8   14   44  155   63   29   99  244   31   37\n",
      "   99   51   29   92    3   48]\n",
      "[ 165  430  581   57 1771 1145 1264  298   87  430   27  702  535  155\n",
      "   86  533   57  197   33  501   58  379  168 1299   90  126  108  703\n",
      "  155  853  179  133  214   52  183  194  515  580   21  407   94  146\n",
      "  111  216   27  648  226  297  112   40  149  315  358  639  396 1763\n",
      "   36   20  167  275  122  247  822  203  736  198  118  125 1306  598\n",
      "  104  152   82  129  403  437  193  151  149  417  217  158  177  180\n",
      " 1141  107  581  351  621  598]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 116   20   29   96   56  383   76    6  138  166    2  168  516   22\n",
      "   81    6   64   24   21  105   13   88    2  131   93   70  113   44\n",
      "  151   81  166   25   72   47    9    9    8   84  293   38   69   14\n",
      "   48   26   14  394  183   50 1082   25   31   91   11   94   61  275\n",
      "    1   58    2  216   55  234   65    4  204    9   64  178  436   48\n",
      "   19   13   32   60    8   14   44  146   82   29   99  244   31   37\n",
      "  214  178   62   91   16   48]\n",
      "[ 165  430  581   57 1771 1145 1264  298   87  430   27  702  535  155\n",
      "   86  533   57  197   33  501   58  379  168 1299   90  126  108  703\n",
      "  155  853  179  133  214   52  183  194  515  580   21  407   94  146\n",
      "  111  216   27  648  226  297  112   40  149  315  358  639  396 1763\n",
      "   36   20  167  275  122  247  822  203  736  198  118  125 1306  598\n",
      "  104  152   82  129  403  437  193  151  149  417  217  158  177  180\n",
      " 1141  107  581  351  621  598]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 116   20   29   94   56  158    5    6  138  176    5   83  516    4\n",
      "   77    6   64   16   21  105   13   57    8 1842   93  125  117   44\n",
      "  151   81   91   25   70   47    9   19    8   84  293   38   69   14\n",
      "   48    9   14  394  183   50 1082   25   28    5   19   94   61  275\n",
      "    1   58    8  130   55  234   46    4   51    9   10  177  401   48\n",
      "   19   13   32   60    8   14   44  146  139   29  129  244   31   18\n",
      "   98   51   29   91    3   48]\n",
      "[ 165  430  581   57 1771 1145 1264  298   87  430   27  702  535  155\n",
      "   86  533   57  197   33  501   58  379  168 1299   90  126  108  703\n",
      "  155  853  179  133  214   52  183  194  515  580   21  407   94  146\n",
      "  111  216   27  648  226  297  112   40  149  315  358  639  396 1763\n",
      "   36   20  167  275  122  247  822  203  736  198  118  125 1306  598\n",
      "  104  152   82  129  403  437  193  151  149  417  217  158  177  180\n",
      " 1141  107  581  351  621  598]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 116   20  263   96   56  383    5    6  138  166    2  169  516    4\n",
      "   77   87   64   16   21  105   13   88   87  386   93  125  113   44\n",
      "  151  252  166   28   72   47    9   10    8   84  293   38   69   14\n",
      "   48    9   14  394  183   74 1082   35   28  256   19   94   61  275\n",
      "   25   58   87  130   55  234   46    4   51    9   64  292  436   48\n",
      "   33   11   32   60    8   14   44  146  139   29  129  244   31   37\n",
      "  214  172  263   91    3   48]\n",
      "[ 165  430  581   57 1771 1145 1264  298   87  430   27  702  535  155\n",
      "   86  533   57  197   33  501   58  379  168 1299   90  126  108  703\n",
      "  155  853  179  133  214   52  183  194  515  580   21  407   94  146\n",
      "  111  216   27  648  226  297  112   40  149  315  358  639  396 1763\n",
      "   36   20  167  275  122  247  822  203  736  198  118  125 1306  598\n",
      "  104  152   82  129  403  437  193  151  149  417  217  158  177  180\n",
      " 1141  107  581  351  621  598]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116  20 334  96  56 383   5   6 138 284   5 198 516   4  77  87  64  16\n",
      "  21 105  13  57 128 446  93 125 117  44 151 252 178  28  72  47   9  12\n",
      "   8  84 293  38  30  14  48  26   5 394 183  74 570  14  28 256  11  94\n",
      "  61 275  25  58 124 130  55 234  42   4 204   9 170 292 436  48  33 159\n",
      "  32  60 120  14  44 146  87  29 129 244  31  16 214 172 275 140   3  48]\n",
      "[ 165  430  581   57 1771 1145 1264  298   87  430   27  702  535  155\n",
      "   86  533   57  197   33  501   58  379  168 1299   90  126  108  703\n",
      "  155  853  179  133  214   52  183  194  515  580   21  407   94  146\n",
      "  111  216   27  648  226  297  112   40  149  315  358  639  396 1763\n",
      "   36   20  167  275  122  247  822  203  736  198  118  125 1306  598\n",
      "  104  152   82  129  403  437  193  151  149  417  217  158  177  180\n",
      " 1141  107  581  351  621  598]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116  20 275  96  56 383   5   6 136 311   5  80 516   4  77  87  64  16\n",
      "  21 105  13   4   1 446  93 125 127   3  85 252 178  28  72  47   9  12\n",
      "   8  84 317  38  30  14  48  26   5 394 183  77 570  14  28   5  19  94\n",
      "  22 276  25  79 124 130  55 234  46   4 204   9  17 126 466  48 104  14\n",
      "  32  60 120  14  44   7  87  29 130 244  31  17  98 126 275 236   3  48]\n",
      "[ 165  430  581   57 1771 1145 1264  298   87  430   27  702  535  155\n",
      "   86  533   57  197   33  501   58  379  168 1299   90  126  108  703\n",
      "  155  853  179  133  214   52  183  194  515  580   21  407   94  146\n",
      "  111  216   27  648  226  297  112   40  149  315  358  639  396 1763\n",
      "   36   20  167  275  122  247  822  203  736  198  118  125 1306  598\n",
      "  104  152   82  129  403  437  193  151  149  417  217  158  177  180\n",
      " 1141  107  581  351  621  598]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116  20 275  92  56 383  88   6 136 311   5  80 516   2 137 145  64  16\n",
      "   5 105  13   4   1 446  93  96 127   3  85 252 202  28  72  47   9  12\n",
      "   8  84 317  38  28  14  48  26   5 394 183  77 570  25  28   5  19  94\n",
      "  22 276  25  60   1 130  55 214  42   4  44   9  17 126 466  48 104  14\n",
      "  32  60   8  14  21   7  87  29 130 252  31  17  99 126 275 236   3  48]\n",
      "[ 165  430  581   57 1771 1145 1264  298   87  430   27  702  535  155\n",
      "   86  533   57  197   33  501   58  379  168 1299   90  126  108  703\n",
      "  155  853  179  133  214   52  183  194  515  580   21  407   94  146\n",
      "  111  216   27  648  226  297  112   40  149  315  358  639  396 1763\n",
      "   36   20  167  275  122  247  822  203  736  198  118  125 1306  598\n",
      "  104  152   82  129  403  437  193  151  149  417  217  158  177  180\n",
      " 1141  107  581  351  621  598]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116  20 275  92  56 383  88   6 136 311   5  80 516 155 137 145  64  16\n",
      "   5 105  13   4   1 386  93  89 110   3 183 252 202  25  70  47   9  12\n",
      "   8 433 317  38  28  14  48  26   5 394 183  77 570  25  28 372  11  94\n",
      "  22 276  25  60   1 130  55 657  42   4 170   9  90 126 466  48 104 159\n",
      "  32  60 120  14  21   7  87  29 130 252  31  17  99 126 275 236   3  48]\n",
      "[ 165  430  581   57 1771 1145 1264  298   87  430   27  702  535  155\n",
      "   86  533   57  197   33  501   58  379  168 1299   90  126  108  703\n",
      "  155  853  179  133  214   52  183  194  515  580   21  407   94  146\n",
      "  111  216   27  648  226  297  112   40  149  315  358  639  396 1763\n",
      "   36   20  167  275  122  247  822  203  736  198  118  125 1306  598\n",
      "  104  152   82  129  403  437  193  151  149  417  217  158  177  180\n",
      " 1141  107  581  351  621  598]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116  20 275  92  56 383  88   6 136 339   5  80 516 155 137 145  64  16\n",
      "   5 105  13   4   1 446  74  89 117   3 151 252 202  25  70  78   9  12\n",
      "  18 433 317  38  28  14  48  26   5 394 183  77 570  39  28 372  11 460\n",
      "  22 276   1  60   1 130  55 657  42   4 170   9  90 126 466  48 104 159\n",
      "  32  56 120  14  21   7  87  29 130 252  31  16  99 126 275 236   3  48]\n",
      "[ 165  430  581   57 1771 1145 1264  298   87  430   27  702  535  155\n",
      "   86  533   57  197   33  501   58  379  168 1299   90  126  108  703\n",
      "  155  853  179  133  214   52  183  194  515  580   21  407   94  146\n",
      "  111  216   27  648  226  297  112   40  149  315  358  639  396 1763\n",
      "   36   20  167  275  122  247  822  203  736  198  118  125 1306  598\n",
      "  104  152   82  129  403  437  193  151  149  417  217  158  177  180\n",
      " 1141  107  581  351  621  598]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116  20 275  92  56 383  88   6 136 311   5  80 516 155 137 145  64  16\n",
      "   5 105  13   4   1 446  74  89 110   3 183 252 202  25  70  78   9  12\n",
      "  18 433 317  38  28  14  48  26   5 394 183  77 570  39  28 372  11 460\n",
      "  22 276   1  60   1 130  55 657  42   4 170   9  90 126 466  48 104 159\n",
      "  32  56 120  14  21   7  87  29 130 252  31  16  99 126 275 236   3  48]\n",
      "[ 165  430  581   57 1771 1145 1264  298   87  430   27  702  535  155\n",
      "   86  533   57  197   33  501   58  379  168 1299   90  126  108  703\n",
      "  155  853  179  133  214   52  183  194  515  580   21  407   94  146\n",
      "  111  216   27  648  226  297  112   40  149  315  358  639  396 1763\n",
      "   36   20  167  275  122  247  822  203  736  198  118  125 1306  598\n",
      "  104  152   82  129  403  437  193  151  149  417  217  158  177  180\n",
      " 1141  107  581  351  621  598]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=6.274795722961426, metrics={'train_runtime': 1838.2028, 'train_samples_per_second': 0.436, 'train_steps_per_second': 0.054, 'total_flos': 3369929015126016.0, 'train_loss': 6.274795722961426, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     overwrite_output_dir=True,\n",
    "#     optim=\"adamw_torch_fused\",\n",
    "#     bf16=True,  # Use BF16 for flash attention\n",
    "#     # evlaution\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=cfg.eval.eval_steps,\n",
    "#     include_inputs_for_metrics=True,\n",
    "#     # logging strategies\n",
    "#     logging_dir=f\"{output_dir}/logs\",\n",
    "#     logging_strategy=\"steps\",\n",
    "#     logging_steps=10,\n",
    "#     save_strategy=\"no\",\n",
    "#     **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    "# ) # TODO: move train arguments to config\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    overwrite_output_dir=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    bf16=True,  # Use BF16 for flash attention\n",
    "    # evlaution\n",
    "    label_names=[\"labels\"], # so that trainer will call compute_loss\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=8,\n",
    "    include_inputs_for_metrics=True,\n",
    "    log_level=\"info\",\n",
    "    # logging strategies\n",
    "    logging_dir=f\"output/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=8,\n",
    "    save_strategy=\"no\",\n",
    "    remove_unused_columns=False,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ") # TODO: move train arguments to config\n",
    "trainer = MultimodalTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=custom_collate,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pix2Struct, reference: https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Invalid key: 997 is out of bounds for size 802",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoProcessor, Pix2StructVisionModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# processor = AutoProcessor.from_pretrained(\"google/pix2struct-base\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model = Pix2StructVisionModel.from_pretrained(\"google/pix2struct-base\")\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m997\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:2800\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:2784\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2783\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2784\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2785\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2786\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2787\u001b[0m )\n\u001b[1;32m   2788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/datasets/formatting/formatting.py:583\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n\u001b[0;32m--> 583\u001b[0m     \u001b[43m_check_valid_index_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# Query the main table\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/scr/wychow/web-scratch/.venv/lib/python3.11/site-packages/datasets/formatting/formatting.py:526\u001b[0m, in \u001b[0;36m_check_valid_index_key\u001b[0;34m(key, size)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m size):\n\u001b[0;32m--> 526\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of bounds for size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n",
      "\u001b[0;31mIndexError\u001b[0m: Invalid key: 997 is out of bounds for size 802"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Pix2StructVisionModel\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"google/pix2struct-base\")\n",
    "# model = Pix2StructVisionModel.from_pretrained(\"google/pix2struct-base\")\n",
    "train_dataset[997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor, Pix2StructVisionModel\n",
    "\n",
    "\n",
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(10))\n",
    "image = train_dataset[3][\"screenshot\"]\n",
    "\n",
    "text = \"A picture of\"\n",
    "\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-textcaps-base\")\n",
    "processor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n",
    "# processor = Pix2StructImageProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n",
    "\n",
    "# image only\n",
    "inputs = processor(images=train_dataset[\"screenshot\"], text=text, return_tensors=\"pt\")\n",
    "print(inputs.keys())\n",
    "predictions = model.generate(**inputs)\n",
    "print(processor.decode(predictions[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pix2Struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pix2StructImageProcessor, Pix2StructVisionModel, Pix2StructConfig, Pix2StructForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(10))\n",
    "image = train_dataset[3][\"screenshot\"]\n",
    "\n",
    "# TODO: Move config to somewhere else\n",
    "image_encoder_path = \"google/pix2struct-textcaps-base\"\n",
    "image_encoder_config = Pix2StructConfig.from_pretrained(image_encoder_path)\n",
    "# TODO: try different hidden size?\n",
    "max_patches = 2000\n",
    "patch_size = 16\n",
    "# image_encoder_config.vision_config.seq_len = max_patches\n",
    "# image_encoder_config.vision_config.patch_size = patch_size\n",
    "print(image_encoder_config)\n",
    "\n",
    "image_encoder = Pix2StructForConditionalGeneration.from_pretrained(image_encoder_path, config=image_encoder_config).encoder\n",
    "print(image_encoder)\n",
    "image_encoder.to(device)\n",
    "\n",
    "processor = Pix2StructImageProcessor.from_pretrained(image_encoder_path) # TODO: define this somewhere else\n",
    "processor.max_patches = max_patches\n",
    "processor.patch_size = {\"height\":patch_size, \"width\":patch_size}\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "print(image_encoder(**inputs))\n",
    "print(torch.cuda.memory_summary())\n",
    "# 2000 -> 7G\n",
    "# 3000 -> 14G\n",
    "# 4000 -> 25G\n",
    "# 5000 -> 37G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoImageProcessor, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(10))\n",
    "image = train_dataset[3][\"screenshot\"]\n",
    "\n",
    "# TODO: Move config to somewhere else\n",
    "image_encoder_path = \"google/vit-base-patch16-224\"\n",
    "image_encoder_config = AutoConfig.from_pretrained(image_encoder_path)\n",
    "# TODO: try different hidden size?\n",
    "# print(image_encoder_config)\n",
    "\n",
    "image_encoder = AutoModel.from_pretrained(image_encoder_path, config=image_encoder_config)\n",
    "# print(image_encoder)\n",
    "image_encoder.to(device)\n",
    "\n",
    "downscale_factor = 4\n",
    "processor = AutoImageProcessor.from_pretrained(image_encoder_path) # TODO: define this somewhere else\n",
    "processor.size = {\"height\":5429//downscale_factor, \"width\":1280//downscale_factor}\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "print(inputs.pixel_values.shape)\n",
    "plt.figure(figsize=(12, 40))\n",
    "plt.imshow(inputs.pixel_values.cpu()[0].permute((1,2,0)))\n",
    "plt.show()\n",
    "h = image_encoder(inputs[\"pixel_values\"], interpolate_pos_encoding=True).last_hidden_state\n",
    "# print(torch.cuda.memory_summary())\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match downscaled image patch index to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[3][\"pos_candidates\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match target index to patch index\n",
    "\n",
    "\n",
    "bounding_box_rect is in the format of (left, bottom, width, height), so pixel_values[:,bottom:bottom+height,left:left+width] should be marked as positive\n",
    "\n",
    "unscaled index 2d -> scaled index 2d -> patch index 2d -> patch index 1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortest width / height:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\")\n",
    "cands = train_dataset[\"pos_candidates\"]\n",
    "shortest = 100\n",
    "widths = []\n",
    "heights = []\n",
    "import json\n",
    "for cand_list in cands:\n",
    "    for cand in cand_list:\n",
    "        json_data = json.loads(cand)\n",
    "        attributes = json.loads(json_data['attributes'])\n",
    "        bounding_box_rect_str = attributes['bounding_box_rect']\n",
    "        lbwh = tuple(map(float, bounding_box_rect_str.split(',')))\n",
    "        widths.append(lbwh[2])\n",
    "        heights.append(lbwh[3])\n",
    "        # if lbwh[2] <= 0 or lbwh[3] <= 0:\n",
    "        #     print(cand_list)\n",
    "        #     print(shortest)\n",
    "\n",
    "        # shortest = min(shortest, lbwh[2], lbwh[3])\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "# plt.hist(widths, bins=100)\n",
    "# plt.show()\n",
    "heights = np.array(heights)\n",
    "plt.hist(heights[heights < 200], bins=100)\n",
    "plt.axvline(x=32, color='r', linestyle='--')\n",
    "plt.title(\"Pos candidates height\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "import torch\n",
    "sample = train_dataset[3]\n",
    "print(sample[\"pos_candidates\"])\n",
    "\n",
    "image = sample[\"screenshot\"]\n",
    "print(image.size)\n",
    "processor = ViTImageProcessor(size={\"height\": 5429, \"width\": 1280})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values # [1, 3, 224, 224]\n",
    "print(pixel_values.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure()\n",
    "plt.imshow(np.transpose(pixel_values[0,:,410:410+46,96:96+106].cpu(), (1,2,0)))\n",
    "\n",
    "\n",
    "processor2 = ViTImageProcessor(size={\"height\": 5429//2, \"width\": 1280//2})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inputs2 = processor2(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values2 = inputs2.pixel_values # [1, 3, 224, 224]\n",
    "print(pixel_values2.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure()\n",
    "plt.imshow(np.transpose(pixel_values2[0,:,410//2:(410+46)//2,96//2:(96+106)//2].cpu(), (1,2,0)))\n",
    "\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.imshow(np.transpose(pixel_values[0].cpu(), (1,2,0)))\n",
    "# for i in range(0, 1000, 100):\n",
    "#     plt.figure()\n",
    "#     plt.imshow(np.transpose(pixel_values[0,:,i:i+160,i:i+160].cpu(), (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def boxes_to_patch_idx_multitarget(box, num_cols):\n",
    "    \"\"\" box is a tensor. Returns a list \"\"\"\n",
    "    # pos_idxs = set()\n",
    "    l, b, w, h = box[0], box[1], box[2], box[3]\n",
    "    # unscaled 2d idx -> scaled 2d idx\n",
    "    x1, x2 = l//2, (l+w)//2\n",
    "    y1, y2 = b//2, (b+h)//2\n",
    "    # scaled 2d idx -> patch 2d idx\n",
    "    x1, x2 = math.floor(x1/16), math.ceil(x2/16)\n",
    "    y1, y2 = math.floor(y1/16), math.ceil(y2/16)\n",
    "    # 2d -> 1d\n",
    "    return [num_cols*r + c for c in range(x1, x2) for r in range(y1, y2)]\n",
    "\n",
    "def boxes_to_patch_idx(box, num_cols):\n",
    "    \"\"\" returns the patch closest to the center of the element \"\"\"\n",
    "    # pos_idxs = set()\n",
    "    l, b, w, h = box[0], box[1], box[2], box[3]\n",
    "    # unscaled 2d idx -> scaled 2d idx\n",
    "    x1, x2 = l//2, (l+w)//2\n",
    "    y1, y2 = b//2, (b+h)//2\n",
    "    # scaled 2d idx -> patch 2d idx\n",
    "    x1, x2 = x1/16, x2/16\n",
    "    y1, y2 = y1/16, y2/16\n",
    "    # 2d -> 1d\n",
    "    c = math.floor((x1+x2)/2)\n",
    "    r = math.floor((y1+y2)/2)\n",
    "    # if x2 - x1 >= 2: # element at least contains 1 whole patch\n",
    "    # else: # element within 2 patches\n",
    "    return num_cols*r + c\n",
    "\n",
    "# for i in range(16):\n",
    "#     print([i*16+j for j in range(16)])\n",
    "print(boxes_to_patch_idx_multitarget([96,410.390625,106,46], 640//16))\n",
    "boxes_to_patch_idx([96,410.390625,106,46], 640//16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoImageProcessor, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"train\").select(range(20))\n",
    "train_dataset[\"screenshot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03c0d0f4f4de45dcb6364473147606b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10bc8d5b2bf14502abf97c47a27ee2b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b59827d64e84940b15ac6e47f5d2e05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23b582e617554b7a874d246e48adf7f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b536d7f4e7b40b38db4c583fcf01a2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2d5cfaa9cbd3499ea81b87e4a2b3ef52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48901010590e483da4289444ec48c40a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d5cfaa9cbd3499ea81b87e4a2b3ef52",
      "placeholder": "​",
      "style": "IPY_MODEL_f06d20046f89484e9180562d0a115aee",
      "value": " 346M/346M [00:03&lt;00:00, 93.3MB/s]"
     }
    },
    "62305803798b4130899d2e93a3b1a5e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03c0d0f4f4de45dcb6364473147606b0",
      "max": 346351599,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b536d7f4e7b40b38db4c583fcf01a2f",
      "value": 346351599
     }
    },
    "825a988043724380a053616c711e8401": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23b582e617554b7a874d246e48adf7f7",
      "placeholder": "​",
      "style": "IPY_MODEL_c734ae403585464ba94cdf0f37b41ac4",
      "value": " 69.7k/69.7k [00:00&lt;00:00, 1.64MB/s]"
     }
    },
    "8b131288e18946a19cd649f6ca43d2a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "97c5e47863024e3b9a1fb13b9e8868d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98ba59932d084e008bd851cb96927063": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d87d89e25d1346a29fb19bbc9fcffa85",
      "placeholder": "​",
      "style": "IPY_MODEL_ff202effd8964c98b8e2fc5c52264d68",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "9ff2ca6160cd4ad283657bceb6831691": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10bc8d5b2bf14502abf97c47a27ee2b7",
      "max": 69665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b131288e18946a19cd649f6ca43d2a9",
      "value": 69665
     }
    },
    "c734ae403585464ba94cdf0f37b41ac4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d09292f5afc045cba07318db5e171471": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d87d89e25d1346a29fb19bbc9fcffa85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "deb144cf25e74807b3adb4fd0fc6ba26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f1c630d0443b44c484db793403db5713",
       "IPY_MODEL_62305803798b4130899d2e93a3b1a5e2",
       "IPY_MODEL_48901010590e483da4289444ec48c40a"
      ],
      "layout": "IPY_MODEL_d09292f5afc045cba07318db5e171471"
     }
    },
    "e7d1c5825ecc466493d5da9633b635c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f06d20046f89484e9180562d0a115aee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f1c630d0443b44c484db793403db5713": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7d1c5825ecc466493d5da9633b635c8",
      "placeholder": "​",
      "style": "IPY_MODEL_97c5e47863024e3b9a1fb13b9e8868d9",
      "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
     }
    },
    "f615c90dfc314e0294b2e32b81f07b5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_98ba59932d084e008bd851cb96927063",
       "IPY_MODEL_9ff2ca6160cd4ad283657bceb6831691",
       "IPY_MODEL_825a988043724380a053616c711e8401"
      ],
      "layout": "IPY_MODEL_1b59827d64e84940b15ac6e47f5d2e05"
     }
    },
    "ff202effd8964c98b8e2fc5c52264d68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
